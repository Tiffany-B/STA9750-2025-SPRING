[
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini Project 3",
    "section": "",
    "text": "Echoes of the Boulevard is a collection of 12 songs that expresses the emotional journey of a person going thorugh the motions of life as waves of ups and down. It bridges the nostalgic notes of Coldplay with the rebellious attitudes of Green Day. It’s your go to playlist for when you are staring out the window thinking about life, or for late night walks on a boulevard. In this playlist, you will find ballads of longing, sing-along anthems, powerful chords and melodic piano notes… Most importantly, you will find that you identify with the songs on a personal level.\n\n\n\n\n\n\n\n\nUsing a statistics-based approach, we explored trends in popularity, danceability, energy, valence, track length, and more. By leveraging summary statistics, correlations, and visualizations, we identified key patterns that guided the creation of our desired playlist. The result is a carefully curated selection of tracks that authentically capture the vibe we aimed to achieve.\n\n\n\n\n\n\nBy employing a heuristic-based approach, the selection of songs for the playlist became more precise and intentional. The heuristics used include several key factors: songs that frequently co-occur in playlists alongside anchor tracks, tracks with similar key and tempo, selections from the same artists, songs with acousticness and danceability values within a defined range, and finally, tracks with a comparable valence (emotional tone). By integrating multiple heuristics, this approach effectively curates a playlist where musical and emotional cohesion is maintained, creating a multi-faceted filtering system that enhances meaningful discovery.\n\n\n\n\n\n\nBy prioritizing sonic cohesion, the playlist achieves a well-curated feel. Tracks are ordered based on valence (mood) and energy, ensuring a consistent emotional and rhythmic experience throughout. This approach maintains uniformity in both mood and tempo, enhancing the overall listening journey. Additionally, the visualization of the playlist serves as a validation of its progression, effectively illustrating the evolution of energy, danceability, and emotional tone.\n\n\n\n\n\n\n\nFor this project, we will examine the key factors that influence a song’s popularity in playlists. Our analysis will utilize the Spotify Million Playlist Dataset, a comprehensive collection of playlists and their associated tracks. Additionally, we will integrate a dataset containing Spotify songs and their audio features to enhance our insights. The table below presents the results of the data import and cleaning process, laying the groundwork for a thorough exploration of song popularity dynamics.\n\n\n\n\nCode\nensure_package &lt;- function(pkg) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n  }\n  library(pkg, character.only = TRUE)\n}\n\n# Ensure the necessary packages are installed and loaded\nensure_package(\"dplyr\")\nensure_package(\"stringr\")\nensure_package(\"tidyr\")\nensure_package(\"httr2\")\nensure_package(\"rvest\")\nensure_package(\"datasets\")\nensure_package(\"purrr\")\nensure_package(\"DT\")\nensure_package(\"jsonlite\")\nensure_package(\"httr\")\nensure_package(\"knitr\")\n\n#Here we will load the song data from a CSV file (posted by Github User gabminamendez) \nload_songs &lt;- function() {\n  # Define file and directory paths\n  directory &lt;- \"data/mp03\"\n  file_url &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n  file_path &lt;- file.path(directory, \"data.csv\")\n  \n  # Create the directory if it doesn't exist\n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n  \n  # Download the file if it doesn't already exist\n  if (!file.exists(file_path)) {\n    download.file(file_url, file_path)\n    message(\"File downloaded: \", file_path)\n  } else {\n    message(\"File already exists: \", file_path)\n  }\n  \n  # Read the CSV file into a data frame\n  song_data &lt;- read.csv(file_path, stringsAsFactors = FALSE)\n  \n  # Process the artists column\n  clean_artist_string &lt;- function(x){\n    str_replace_all(x, \"\\\\['\", \"\") |&gt; \n      str_replace_all(\"'\\\\]\", \"\") |&gt;\n      str_replace_all(\" '\", \"\")\n  }\n  \n  # Process and return the data\n  processed_data &lt;- song_data |&gt; \n    separate_longer_delim(artists, \",\") |&gt;\n    mutate(artist = clean_artist_string(artists)) |&gt;\n    select(-artists)\n  \n  return(processed_data)\n}\n\n# Now actually call the function and save the result\nsong_data &lt;- load_songs()\n\n# View the first few rows to confirm it worked\n# head(song_data)\n# names(song_data)\n# \n# Next, we will load the playlist data from a JSON file (posted by Github User DevinOgrady)\n# Define the function to load the playlist data from the repository, store them locally, and return a tidy data frame to read them into R in a standardized format.\n\nload_playlist &lt;- function() {\n  p_directory &lt;- \"data/mp03/playlists\"\n  p_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/master/data1/\"\n  j_file_name &lt;- \"mpd.slice.0-999.json\"\n  file_path_1 &lt;- file.path(p_directory, j_file_name)\n  file_url_1 &lt;- paste0(p_url, j_file_name)\n  \n  if (!dir.exists(p_directory)) {\n    dir.create(p_directory, recursive = TRUE)\n  }\n  \n  if (!file.exists(file_path_1)) {\n    download.file(file_url_1, destfile = file_path_1, method = \"auto\")\n  }\n  \n  playlist_data &lt;- fromJSON(file_path_1, simplifyVector = FALSE)  \n  \n  if (!\"playlists\" %in% names(playlist_data)) return(NULL)\n  \n  playlist_tracks &lt;- map_dfr(playlist_data$playlists, function(pl) {\n    if (!is.list(pl)) return(NULL)\n    if (!(\"tracks\" %in% names(pl))) return(NULL)\n    if (!is.list(pl$tracks) || length(pl$tracks) == 0) return(NULL)\n    \n    tidy_data &lt;- tryCatch(\n      bind_rows(pl$tracks),\n      error = function(e) NULL\n    )\n    \n    if (is.null(tidy_data) || nrow(tidy_data) == 0) return(NULL)\n    \n    tidy_data |&gt;\n      mutate(\n        playlist_name      = pl$name,\n        playlist_id        = pl$pid,\n        playlist_followers = pl$num_followers,\n        playlist_position  = row_number()\n      ) |&gt;\n      select(\n        playlist_name,\n        playlist_id,\n        playlist_followers,\n        playlist_position,\n        track_name,\n        track_uri,\n        artist_name,\n        artist_uri,\n        album_name,\n        album_uri,\n        duration_ms\n      ) |&gt;\n      rename(\n        track_id  = track_uri,\n        artist_id = artist_uri,\n        album_id  = album_uri,\n        duration  = duration_ms\n      )\n  })\n  \n  return(playlist_tracks)\n}\n\n# Load playlists data\nplaylists_data &lt;- load_playlist()\n# glimpse(playlists_data)\n# head(playlists_data)\n# View(playlists_data)\n\n#‘Rectangle’ the Playlist Data\n#Convert the playlist data into the rectangle format, which is a tidy data frame with one row per track in a playlist.\nstrip_spotify_prefix &lt;- function(x){\n  library(stringr)\n  str_extract(x, \".*:.*:(.*)\", group=1)\n}\n\nplaylists_data &lt;- playlists_data |&gt;\n  mutate(\n    track_id  = sapply(track_id, strip_spotify_prefix),\n    artist_id = sapply(artist_id, strip_spotify_prefix),\n    album_id  = sapply(album_id, strip_spotify_prefix)\n  )\n# View(playlists_data)\n\n#create a kabble of the first 10 rows of the playlist data\nkable(playlists_data[1:10,])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplaylist_name\nplaylist_id\nplaylist_followers\nplaylist_position\ntrack_name\ntrack_id\nartist_name\nartist_id\nalbum_name\nalbum_id\nduration\n\n\n\n\nThrowbacks\n0\n1\n1\nLose Control (feat. Ciara & Fat Man Scoop)\n0UaMYEvWZi0ZqiDOoHU3YI\nMissy Elliott\n2wIVse2owClT7go1WT98tk\nThe Cookbook\n6vV5UrXcfyQD1wu4Qo2I9K\n226863\n\n\nThrowbacks\n0\n1\n2\nToxic\n6I9VzXrHxO9rA9A5euc8Ak\nBritney Spears\n26dSoYclwsYLMAKD3tpOr4\nIn The Zone\n0z7pVBGOD7HCIB7S8eLkLI\n198800\n\n\nThrowbacks\n0\n1\n3\nCrazy In Love\n0WqIKmW4BTrj3eJFmnCKMv\nBeyoncé\n6vWDO969PvNqNYHIOW5v0m\nDangerously In Love (Alben für die Ewigkeit)\n25hVFAxTlDvXbx2X2QkUkE\n235933\n\n\nThrowbacks\n0\n1\n4\nRock Your Body\n1AWQoqb9bSvzTjaLralEkT\nJustin Timberlake\n31TPClRtHm23RisEBtV3X7\nJustified\n6QPkyl04rXwTGlGlcYaRoW\n267266\n\n\nThrowbacks\n0\n1\n5\nIt Wasn’t Me\n1lzr43nnXAijIGYnCT8M8H\nShaggy\n5EvFsr3kj42KNv97ZEnqij\nHot Shot\n6NmFmPX56pcLBOFMhIiKvF\n227600\n\n\nThrowbacks\n0\n1\n6\nYeah!\n0XUfyU2QviPAs6bxSpXYG4\nUsher\n23zg3TcAtWQy7J6upgbUnj\nConfessions\n0vO0b1AvY49CPQyVisJLj0\n250373\n\n\nThrowbacks\n0\n1\n7\nMy Boo\n68vgtRHr7iZHpzGpon6Jlo\nUsher\n23zg3TcAtWQy7J6upgbUnj\nConfessions\n1RM6MGv6bcl6NrAG8PGoZk\n223440\n\n\nThrowbacks\n0\n1\n8\nButtons\n3BxWKCI06eQ5Od8TY2JBeA\nThe Pussycat Dolls\n6wPhSqRtPu1UhRCDX5yaDJ\nPCD\n5x8e8UcCeOgrOzSnDGuPye\n225560\n\n\nThrowbacks\n0\n1\n9\nSay My Name\n7H6ev70Weq6DdpZyyTmUXk\nDestiny’s Child\n1Y8cdNmUJH7yBTd9yOvr5i\nThe Writing’s On The Wall\n283NWqNsCA9GwVHrJk59CG\n271333\n\n\nThrowbacks\n0\n1\n10\nHey Ya! - Radio Mix / Club Mix\n2PpruBYCo4H7WOBJ7Q2EwM\nOutKast\n1G9G7WwrXka3Z1r7aIDjI7\nSpeakerboxxx/The Love Below\n1UsmQ3bpJTyK6ygoOOjG1r\n235213\n\n\n\n\n\n\n\n\n\n\nWith a imported and cleaned dataset, we can now begin to explore the data.\n\n\nHow many distinct tracks and artists are represented in the playlist data?\n\nThere are 9,745 distinct artists and 34,443 distinct tracks in the playlist data.The following table shows the distinct artists in the playlist.\n\n\nCode\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\n#Count the number of distinct artists and tracks\ndistinct_artists &lt;- playlists_data |&gt;\n  distinct(artist_id) |&gt;\n  summarise(\"Number of Artists\" = n())\n#print(distinct_artists)\n\n#create a kable of distinct artists\nkable(distinct_artists)\n\n\n\n\n\nNumber of Artists\n\n\n\n\n9754\n\n\n\n\n\n The following table shows the distinct tracks in the playlist data set.\n\n\nCode\n#Count the number of distinct tracks\ndistinct_tracks &lt;- playlists_data |&gt;\n  distinct(track_id) |&gt;\n  summarise(\"Number of Tracks\" = n())\n#print(distinct_tracks)\n\n#create a kable of distinct tracks\nkable(distinct_tracks)\n\n\n\n\n\nNumber of Tracks\n\n\n\n\n34443\n\n\n\n\n\n\n\nWhat are the 5 most popular tracks in the playlist data?\nThe most popular track is Closer which made 75 appearances in the playlist data.\n\n The following table shows the 5 most popular tracks in the playlist data.\n\n\nCode\n# Find the most popular tracks\nmost_popular_tracks &lt;- playlists_data |&gt;\n  group_by(track_name) |&gt;\n  summarise(`Number of Appearances` = n()) |&gt;\n  arrange(desc(`Number of Appearances`)) |&gt;\n  slice(1:5) |&gt;\n  rename(\"Name of Track\" = track_name) \n#print(most_popular_tracks)\n#View(most_popular_tracks)\n\n# Create a kable of the most popular tracks\nkable(most_popular_tracks)\n\n\n\n\n\nName of Track\nNumber of Appearances\n\n\n\n\nCloser\n75\n\n\nOne Dance\n55\n\n\nHUMBLE.\n52\n\n\nRide\n52\n\n\nBroccoli (feat. Lil Yachty)\n50\n\n\n\n\n\n\n\nWhat is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\nThe most popular track that does not have a corresponding entry in the song characteristics data is One Dance by Drake. It appears 55 times in the playlist data and is shockingly not in the song characteristics data set for it’s danceability.\n\n\n\n\nCode\n#join data frames by track id column\nsong_data &lt;- song_data |&gt;\n  rename(\"track_id\" = id)\n\n#Find the most popular track that has no entry in song  data\nno_match_tracks &lt;- playlists_data |&gt;\n  anti_join(song_data, by = \"track_id\") \n\nmost_pop_nochar &lt;- (\n  no_match_tracks |&gt;\n    count(`Name of Track`= track_name, `Artist` = artist_name, sort = TRUE) |&gt;\n    slice_max(n, n = 1) |&gt;\n    rename(`Number of Appearances` = n)\n)\n\n#View(most_pop_nochar)\n\n# Create a kable of the most popular track without characteristics\nkable(most_pop_nochar)\n\n\n\n\n\nName of Track\nArtist\nNumber of Appearances\n\n\n\n\nOne Dance\nDrake\n55\n\n\n\n\n\n 4. According to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\nThe most danceable track is Tone-Loc's Funky Cold Medina. It appears one time in the playlist titled ***VACATION*** and has a danceability score that is closest to 1.\n\n\nCode\n# use left join to combine the song data with the playlist data\njoin_data &lt;- song_data |&gt;\n  left_join(playlists_data, by = \"track_id\")\n\n# Find the most danceable track\nmost_danceable_track &lt;- join_data |&gt;\n  select(track_name, artist_name, danceability, playlist_name) |&gt;\n  arrange(desc(danceability)) |&gt;\n  rename(\"Name of Track\" = track_name, \"Artist\" = artist_name, \"Danceability\" = danceability, \"Playlist Name\" = playlist_name) |&gt;\n  slice(1)\n\n#View(most_danceable_track)\n\n# Create a kable table to display the most danceable track\nkable(most_danceable_track)\n\n\n\n\n\nName of Track\nArtist\nDanceability\nPlaylist Name\n\n\n\n\nFunky Cold Medina\nTone-Loc\n0.988\nVACATION\n\n\n\n\n\n\n\nWhich playlist has the longest average track length?\nThe playlist with the longest average track length is the Classical playlist. It’s average track length is approximately 411148.7 milliseconds or 7 minutes.\n\n\n\nCode\n# Find the playlist with the longest average track length by grouping by playlist name \n# and calculating the average track length (average of the duration column).\n# Then, calculate the average length in minutes and sort the data frame in descending order.\n#names(playlists_data)\nlongest_playlist &lt;- playlists_data |&gt;\n  group_by(playlist_name) |&gt;\n  summarise(avg_length = mean(duration)) |&gt;\n  mutate(avg_length_min = round(avg_length/60000, 0)) |&gt;\n  arrange(desc(avg_length)) |&gt;\n  rename(\"Playlist Name\" = playlist_name, \"Average Track Length (ms)\" = avg_length, \"Average Track Length (min)\" = avg_length_min) |&gt;\n  slice(1)\n#View(longest_playlist)\n\n# Create a kable Extra table to display the playlist with the longest average track length\nkable(longest_playlist)\n\n\n\n\n\nPlaylist Name\nAverage Track Length (ms)\nAverage Track Length (min)\n\n\n\n\nclassical\n411148.7\n7\n\n\n\n\n\n\n\nWhat is the most popular playlist on Spotify?\nThe most popular playlist on Spotify is Tangled with 1038 followers.\n\n\n\nCode\n#Find the most popular playlist by grouping by playlist id and name and number of followers\nmost_popular_playlist &lt;- playlists_data |&gt;\n  distinct(playlist_id, playlist_name, playlist_followers) |&gt;\n  slice_max(playlist_followers) |&gt;\n  rename(\"Playlist ID\" = playlist_id,\"Playlist Name\" = playlist_name, \"Number of Followers\" = playlist_followers) \n#View(most_popular_playlist)\n\n# Create a kable table to display the most popular playlist\n\nkable(most_popular_playlist)\n\n\n\n\n\nPlaylist ID\nPlaylist Name\nNumber of Followers\n\n\n\n\n765\nTangled\n1038\n\n\n\n\n\n\n\n\n\n\nNext, we will conduct a more in-depth analysis of the most popular songs in the data set using INNER_JOIN to combine the song data with the playlist data set. We will use ggplot2 to visualize the data.\n\n\nIs the popularity column correlated with the number of playlist appearances? If so, to what degree?\nThe following plot shows the correlation between the popularity and the number of playlist appearances. The correlation is positive, indicating that as the popularity increases, the number of playlist appearances also increases. The thresholds are set at 0-25 for Low and 25-75 for Medium and 75-100 for High for mean popularity.\n\n\n\n\nCode\n#use inner join to combine the song data with the playlist data\ninner_join_data &lt;- song_data |&gt;\n  inner_join(playlists_data, by = \"track_id\")\n\n#using the inner join data find the correlation between the popularity and the number of playlist appearances\npop_correlation &lt;- inner_join_data |&gt;\n  group_by(track_id, track_name) |&gt;\n  summarise(num_of_playlists = n(),\n            mean_popularity = mean(popularity),\n            .groups = \"drop\") \n\n#create a new column to categorize the popularity into groups (Low, Medium, High)\n\npop_correlation &lt;- pop_correlation |&gt;\n  mutate(popularity_group = cut(\n    mean_popularity,\n    breaks = c(0, 25, 75, 100),\n    labels = c(\"Low\", \"Medium\", \"High\"),\n    include.lowest = TRUE\n  ))\n#View(pop_correlation)\n\n# Create a boxplot to display the popularity groups and their average playlist appearances\nlibrary(ggplot2)\nggplot(pop_correlation, aes(x = popularity_group, y = num_of_playlists)) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"blue\") +\n  labs(\n    title = \"Playlist Appearances by Popularity Group\",\n    x = \"Popularity Group\",\n    y = \"Number of Playlist Appearances\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nIn what year were the most popular songs released?.\nThe most popular songs were released in 2017. The highest average popularity of released songs was approximately 70.79.\n\n\n\nCode\n#Using the inner join data and mean, we will filter the data to find the most popular songs released in a distinct year.\npop_year &lt;- inner_join_data |&gt;\n  group_by(year) |&gt;\n  summarise(mean_pop_songs = mean(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(mean_pop_songs))\n\n#head(pop_year)\ntop_ten_pop_year &lt;- pop_year |&gt;\n  slice_head(n=10)\n#create kable of the most popular songs released in a distinct year\nkable(top_ten_pop_year) \n\n\n\n\n\nyear\nmean_pop_songs\n\n\n\n\n2017\n70.78972\n\n\n1976\n69.37306\n\n\n2016\n67.92846\n\n\n1983\n67.48315\n\n\n1982\n67.05217\n\n\n1980\n66.93662\n\n\n1981\n66.90955\n\n\n1979\n66.62581\n\n\n1967\n66.18493\n\n\n1973\n66.00585\n\n\n\n\n\nCode\n#View(pop_year)\n\n#visualize the average popularity of songs by year\npop_year_plot &lt;- ggplot(pop_year, aes(x = year, y = mean_pop_songs)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"purple\") +\n  labs(title = \" Average Popularity of Released Songs by Year\",\n       x = \"Year\",\n       y = \"Average Popularity\")\nprint(pop_year_plot)\n\n\n\n\n\n\n\n\n\n\n\nIn what year did danceability peak?\nThe year with the highest average danceability is 2017. The average danceability for that year is 0.71.\n\n\n\nCode\n# Using the inner join data, we will filter the data to find the year with the highest average danceability.\ndanceability_year &lt;- inner_join_data |&gt;\n  group_by(year) |&gt;\n  summarise(mean_danceability = mean(danceability, na.rm = TRUE)) |&gt;\n  slice_max(mean_danceability, n = 1) |&gt;\n  rename(\"Average Danceability\" = mean_danceability)\n  \n#View(danceability_year)\n\n#create a kable of the year with the highest average danceability\n\nkable(danceability_year)\n\n\n\n\n\nyear\nAverage Danceability\n\n\n\n\n2017\n0.7120776\n\n\n\n\n\nCode\n#data to view all average danceability by year\ndanceability_year_all &lt;- inner_join_data |&gt;\n  group_by(year) |&gt;\n  summarise(mean_danceability = mean(danceability, na.rm = TRUE)) |&gt;\n  arrange(desc(mean_danceability)) \n\n#visualize the average danceability of songs by year using a line plot\ndanceability_year_plot &lt;- ggplot(danceability_year_all, aes(x = year, y = mean_danceability)) +\n  geom_line(color = \"deepskyblue\") +\n  geom_point(color = \"black\") +\n  labs(title = \"Average Danceability by Year\",\n       x = \"Year\",\n       y = \"Average Danceability\")\nprint(danceability_year_plot)\n\n\n\n\n\n\n\n\n\n\n\nWhich decade is most represented on user playlists?\nThe decade with the most songs is the 2010s. The number of 2010 songs on most users playlist at that time was 24,713.\n\n\n\nCode\n#Mutate a column to calculate the decade using integer division\npop_decade &lt;- inner_join_data |&gt;\n  mutate(decade = (year %/% 10) * 10) |&gt;\n  group_by(decade) |&gt;\n  summarise(num_songs = n()) |&gt;\n  arrange(desc(num_songs)) |&gt;\n  rename(\"Number of Songs\" = num_songs, \"Decade\" = decade)\n\n#View(pop_decade)\n# Create a kable Extra table to display the number of songs by decade\nkable(pop_decade)\n\n\n\n\n\nDecade\nNumber of Songs\n\n\n\n\n2010\n24713\n\n\n2000\n7291\n\n\n1990\n3069\n\n\n1980\n1573\n\n\n1970\n1501\n\n\n1960\n735\n\n\n1950\n93\n\n\n1940\n38\n\n\n1930\n2\n\n\n\n\n\nCode\n# Create a bar plot to visualize the number of songs by decade\npop_decade_plot &lt;- ggplot(pop_decade, aes(x = factor(`Decade`), y = `Number of Songs`)) +\n  geom_bar(stat = \"identity\", fill = \"purple\", color = \"black\") +\n  labs(title = \"Number of Songs by Decade\",\n       x = \"Decade\",\n       y = \"Number of Songs\") +\n  theme_minimal()\n\nprint(pop_decade_plot)\n\n\n\n\n\n\n\n\n\n\n\nPlotting the key frequency among songs\nThe most common key is the 0 and 1 keys. The frequency of those key is more than 1,000.\n\n\n\nCode\n# Calculate the frequency of each key\nkey_frequency &lt;- inner_join_data |&gt;\n  group_by(key) |&gt;\n  summarize(count = n()) |&gt;\n  arrange(desc(count))\n\n# Create a polar plot...\nggplot(key_frequency, aes(x = as.factor(key), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\", color = \"black\") +\n  coord_polar(start = 0) +\n  theme_bw() +\n  labs(title = \"Frequency of Musical Keys\",\n       x = \"Key\",\n       y = \"Count of Keys\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14),\n        plot.title = element_text(size = 16))\n\n\n\n\n\n\n\n\n\n\n\nWhat are the most popular track lengths?\nThe following histogram shows the distribution of track lengths. The most common track length is between 2 and 4 minutes.\n\n\n\nCode\n#using the inner join data, we will filter the data to find the most popular track lengths (in minutes)\ninner_join_data&lt;- inner_join_data |&gt;\n  mutate(track_length = duration/ 60000)\n\n#find the mean, median, min, and max track lengths\ntrack_length_data &lt;- inner_join_data |&gt;\n  summarise(\n    avg_length = mean(track_length, na.rm = TRUE),\n    median_length = median(track_length, na.rm = TRUE),\n    min_length = min(track_length, na.rm = TRUE),\n    max_length = max(track_length, na.rm = TRUE)\n  ) \n#View(track_length_data)\n#Create a kable table to display the track length data\nkable(track_length_data)\n\n\n\n\n\navg_length\nmedian_length\nmin_length\nmax_length\n\n\n\n\n3.974142\n3.833333\n0.0034333\n37.31222\n\n\n\n\n\nCode\n#Create a histogram to visualize the distribution of track lengths\nggplot(inner_join_data, aes(x = track_length)) +\n  geom_histogram(binwidth = 0.6, fill = \"cornflowerblue\", color = \"black\") +\n  labs(title = \"Most Popular Track Lengths\",\n       x = \"Track Length (minutes)\",\n       y = \"Number of Songs\") +\n  theme_bw() +\n  scale_x_continuous(\n    limits = c(0, 10),       # Only min and max go here\n    breaks = seq(0, 10, 2)   # Add ticks every 2 minutes\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that exploring the data is finished we can build the playlist! First, we picked two anchor songs from the playlist data set: Wake Me Up When September Ends by Green Day and Fix You by Coldplay. Then, we found songs that work well in the playlist using heuristics.\n\n\n\n\n\nWhat other songs commonly appear on playlists along side this song?\n\n\n\nCode\n#Using inner join data we will filter the data to find the anchor songs and their characteristics. \nanchor_songs &lt;- inner_join_data |&gt;\n  filter(track_name %in% c(\"Wake Me Up When September Ends\", \"Fix You\")) |&gt; \n  distinct(track_id, track_name, artist_name, key, tempo, year, popularity, acousticness, danceability, valence, instrumentalness,energy)\n#View(anchor_songs)\n\n#distinct(inner_join_data, track_name) |&gt; View()\n\n#Heuristic 1: Songs that Commonly Appear with the Anchor Song\n#We'll find songs that appeared in the same playlists as the anchor song(s).\n\nanchor_ids &lt;- anchor_songs$track_id\n\nco_occurring_tracks &lt;- playlists_data |&gt;\n  filter(track_id %in% anchor_ids) |&gt;\n  select(playlist_id) |&gt;\n  inner_join(playlists_data, by = \"playlist_id\") |&gt;\n  filter(!track_id %in% anchor_ids) |&gt;  # exclude the anchor itself\n  count(track_name, artist_name, sort = TRUE) |&gt;\n  slice_max(n, n = 20)\n# Rename columns for clarity\n\nco_occurring_tracks &lt;- co_occurring_tracks |&gt;\n  rename(\n    `Track Name` = track_name,\n    `Artist Name` = artist_name,\n    `Number of Playlists` = n\n  )\n\n#View(co_occurring_tracks)\ntop_ten_co_ocurring &lt;- co_occurring_tracks |&gt;\n  slice_head(n = 10)\n# Create a kable table to display the similar co-occuring tracks\nkable(top_ten_co_ocurring) \n\n\n\n\n\n\n\n\n\n\nTrack Name\nArtist Name\nNumber of Playlists\n\n\n\n\nChasing Cars\nSnow Patrol\n9\n\n\nDrops of Jupiter\nTrain\n8\n\n\nI Write Sins Not Tragedies\nPanic! At The Disco\n7\n\n\nYellow\nColdplay\n7\n\n\n21 Guns\nGreen Day\n6\n\n\nMr. Brightside\nThe Killers\n6\n\n\nShe Will Be Loved - Radio Mix\nMaroon 5\n6\n\n\nThe Scientist\nColdplay\n6\n\n\nHoliday/Boulevard Of Broken Dreams\nGreen Day\n5\n\n\nHow to Save a Life\nThe Fray\n5\n\n\n\n\n\n\n\nWhat other songs are in the same key and have a similar tempo?\n\nThe following table shows the songs that are in the same key and have a similar tempo to the anchor songs. The tempo is between 5 BPM of the anchor songs.\n\n\nCode\n################\n#Heuristic 2: Same Key + Similar Tempo (±5 BPM)\nlibrary(purrr)\nsimilar_key_tempo &lt;- inner_join_data |&gt;\n  filter(\n    key %in% anchor_songs$key,\n    map_lgl(tempo, ~ any(abs(.x - anchor_songs$tempo) &lt;= 5))\n  ) |&gt;\n  distinct(track_id, track_name, artist_name, tempo, key)|&gt;\n  slice_max(tempo, n = 15) # Get top 10 similar tracks\n\n# Rename columns for clarity\nsimilar_key_tempo &lt;- similar_key_tempo |&gt;\n  rename(\n    `Track Name` = track_name,\n    `Artist Name` = artist_name,\n    `Tempo (BPM)` = tempo,\n    `Key` = key\n  )\n\n#View(similar_key_tempo)\ntop_ten_keytemp &lt;- similar_key_tempo  |&gt;\n  slice_head(n = 10)\n# Create a kable table to display the similar key tempo tracks\nkable(top_ten_keytemp) \n\n\n\n\n\n\n\n\n\n\n\n\ntrack_id\nTrack Name\nArtist Name\nTempo (BPM)\nKey\n\n\n\n\n71tjsDvB4EMJqNG8EMmFnb\nComin’ Home Baby\nMel Tormé\n143.121\n7\n\n\n1f2lkuLldqRzvaFjJdjGXX\nSanta Claus Is Comin’ to Town\nMariah Carey\n143.109\n7\n\n\n4NTWZqvfQTlOMitlVn6tew\nThe Show Goes On\nLupe Fiasco\n143.067\n7\n\n\n10eDxSTjwMBq1ZjZK5b1cK\nMillennia\nCrown The Empire\n143.036\n3\n\n\n2rUwQj4SWaP2anuGDtNpYR\nSelf-Made\nBryson Tiller\n143.001\n7\n\n\n0UDCfleTgwihlnOUxbzokR\nAwake\nBTS\n142.816\n3\n\n\n6NGi23FFKq9tH5NR1NcTw2\nLet Her Cry\nHootie & The Blowfish\n142.570\n7\n\n\n3403qFGo7u2ptUyJbdEkjI\nFor the Widows in Paradise, For the Fatherless in Ypsilanti\nSufjan Stevens\n142.433\n3\n\n\n4F55RCGuM477OjznpYGhYz\nShortie Like Mine\nBow Wow\n142.149\n7\n\n\n7g8OpS827dAYU067lZaR0L\nJust a Dream\nCarrie Underwood\n142.060\n7\n\n\n\n\n\n\n\nWhat other songs were released by the same artists?\n\n\n\nCode\n#Heuristic 3: Same Artist\nsame_artist_tracks &lt;- inner_join_data |&gt;\n  filter(artist_name %in% anchor_songs$artist_name) |&gt;\n  distinct(track_id, track_name, artist_name)\n\n#View(same_artist_tracks)\n# Create a kable Extra table to display the same artist tracks\n\ntop_ten_same &lt;- same_artist_tracks |&gt;\n  slice_head(n = 10)\n\nkable(top_ten_same) \n\n\n\n\n\ntrack_id\ntrack_name\nartist_name\n\n\n\n\n31L9yLXSj6LpCFupyMV6CR\nUp&Up\nColdplay\n\n\n5qfZRNjt2TkHEL12r3sDEU\nEverglow\nColdplay\n\n\n4ZcnZVXwLDLWI93SLJER3a\nMisery\nGreen Day\n\n\n1rkbMXhEjIytsUGbhoR5pn\nLife In Technicolor\nColdplay\n\n\n1qIgyDoc2rwtq8w49jeWL8\nOh Love\nGreen Day\n\n\n6f49kbOuQSOsStBpyGvQfA\nA Head Full Of Dreams\nColdplay\n\n\n6FnDerFHdaeCFovZnQ3r14\nLast Ride In\nGreen Day\n\n\n3KzCJGegAcwsSik1bOgkNu\nWords I Might Have Ate\nGreen Day\n\n\n6c6W25YoDGjTq3qSPOga5t\nInk\nColdplay\n\n\n3HWDWyIqWuLsTHECx9DvXF\nBirds\nColdplay\n\n\n\n\n\n\n\nWhat other songs were released in the same year and have similar levels of acousticness and danceability?\n\nWe’ll allow a range (±0.1) around the acousticness/danceability of the anchor songs.\n\n\nCode\n# heuristic 4 - \n# We'll allow a range (±0.1) around the acousticness/danceability of the anchor songs.\n#find the range of years for the anchor songs\n#and filter the data to find the similar songs\n\n\nanchor_range &lt;- anchor_songs |&gt;\n  summarise(\n    min_acoustic = min(acousticness) - 0.1,\n    max_acoustic = max(acousticness) + 0.1,\n    min_dance = min(danceability) - 0.1,\n    max_dance = max(danceability) + 0.1,\n    year_range = list(unique(year))  # make year_range a list-column\n  )\n\n# Pull the scalar values from the 1-row data frame\nmin_acoustic &lt;- anchor_range$min_acoustic\nmax_acoustic &lt;- anchor_range$max_acoustic\nmin_dance &lt;- anchor_range$min_dance\nmax_dance &lt;- anchor_range$max_dance\nyear_range &lt;- anchor_range$year_range[[1]]  # unlist the year_range safely\n\n# Now filter safely\nsimilar_vibe_tracks &lt;- inner_join_data |&gt;\n  filter(\n    year %in% year_range,\n    acousticness &gt;= min_acoustic,\n    acousticness &lt;= max_acoustic,\n    danceability &gt;= min_dance,\n    danceability &lt;= max_dance\n  ) |&gt;\n  distinct(track_id, track_name, artist_name, year)\n\n\n#View(similar_vibe_tracks)\ntop_ten_vibe &lt;- similar_vibe_tracks |&gt;\n  slice_head(n = 10)\n\n#create a kable table to display the similar vibe tracks\nkable(top_ten_vibe)\n\n\n\n\n\n\n\n\n\n\n\ntrack_id\ntrack_name\nartist_name\nyear\n\n\n\n\n28jOIJzouTp7gqhtx7RbGa\nBlame It On Bad Luck\nBayside\n2005\n\n\n4e4vO6bp5nSdP9G79O3qS6\nMiss Me Baby\nChris Cagle\n2005\n\n\n2YxoC2dYOotjqt08dEifsc\nChromakey Dreamcoat\nBoards of Canada\n2005\n\n\n2itu79WbZhUCHX4jg0fyAd\nNever Let This Go\nParamore\n2005\n\n\n53Qpn8LPa0IhFCTiPO0Bbm\nI Disappear\nThe Faint\n2004\n\n\n0L639McB94IkUvIgZKM1E5\nPirates\nBullets And Octane\n2004\n\n\n5fHkJ1a7v0JFPiT9rFQI8U\nTe Daré Lo Mejor\nJesús Adrián Romero\n2004\n\n\n6u84UiQ3TRczfYTDVYrz7S\nSomeone That You’re With\nNickelback\n2005\n\n\n433JymbpWnRMHXzp1oTRP7\nDon’t Bother\nShakira\n2005\n\n\n4sjjV02cN3nDOIZPJabEgM\nThe Time Is Now\nJohn Cena\n2005\n\n\n\n\n\n\n\nWhat other songs were released have similar levels of valence?\n\n\n\nCode\n#Valence(Mood) or emotional tone of the track\n# names(anchor_songs)\n#We will use the valence column to find similar songs\nvalence_range &lt;- anchor_songs |&gt; summarise(\n  min_valence = min(valence) - 0.001,\n  max_valence = max(valence) + 0.001\n)\n#compare  the valence of the anchor songs to the valence of the other songs\nsimilar_valence_tracks &lt;- inner_join_data |&gt;\n  filter(\n    valence &gt;= valence_range$min_valence,\n    valence &lt;= valence_range$max_valence\n  ) |&gt;\n  distinct(track_id, track_name, artist_name, valence)\n\n# Rename columns for clarity\nsimilar_valence_tracks &lt;- similar_valence_tracks |&gt;\n  rename(\n    `Track Name` = track_name,\n    `Artist Name` = artist_name,\n    `Valence` = valence\n  )\n\n#View(similar_valence_tracks)\ntop_ten_valence &lt;- similar_valence_tracks |&gt;\n  slice_head(n = 10)\n# Create a kable table to display the similar valence tracks\nkable(top_ten_valence) \n\n\n\n\n\n\n\n\n\n\n\ntrack_id\nTrack Name\nArtist Name\nValence\n\n\n\n\n3lpa6fzS3rSAbRogEhWxu7\nStar Spangled Banner - Live at Woodstock\nJimi Hendrix\n0.129\n\n\n1YBf7Tq9bpcVwvnlP8YbQS\nYear Zero\nGhost B.C.\n0.137\n\n\n52WTLETEHs5jwCr7LCq0VW\nQueen\nPerfume Genius\n0.130\n\n\n5qfZRNjt2TkHEL12r3sDEU\nEverglow\nColdplay\n0.136\n\n\n0WCbhE2evMrIwRM0DlMy9k\nMixtape (feat. Young Thug & Lil Yachty)\nChance The Rapper\n0.126\n\n\n5eG8QuQKOBvDeCyoOeMkhT\nAlps\nNovo Amor\n0.135\n\n\n7hmdJhc4W0idVVoMES7F9F\nIf I Know Me\nGeorge Strait\n0.146\n\n\n4qKDjmz094Bu2wMepNuwVN\nMain Title / Once Upon A Dream / Prologue - From “Sleeping Beauty” Soundtrack\nChorus - Sleeping Beauty\n0.143\n\n\n1fckqKAI9ug7U1DgQrrOop\nBelieve It - feat. Rick Ross\nMeek Mill\n0.138\n\n\n0st2C7aLz9xkr7JyshhUHK\nScandal of Grace\nHillsong United\n0.138\n\n\n\n\n\n\n\n\n\n\n\nThe following playlist is comprised of the offical 12 candidates selected for the playlist before ordering. It was created using all four heurtics above to find songs of similar standing as the chosen anchor songs. Additionally, it’s comprised of 2 songs that I was not familiar with, I Disappear and Chromakey Dreamcoat. This means that the playlist is also pretty diverse in terms of popularity.\n\n\n\nCode\n#final Playlist Candidate\n#Now combine all those results and filter down to at least 20 unique songs, making sure at least 8 are not “popular” (say, popularity &lt; 60)\nanchor_artists &lt;- unique(anchor_songs$artist_name)\nanchor_ids &lt;- unique(anchor_songs$track_id)\nall_candidates &lt;- bind_rows(\n  co_occurring_tracks,\n  similar_key_tempo,\n  similar_vibe_tracks,\n  similar_valence_tracks\n) |&gt;\n  filter(!artist_name %in% anchor_artists) |&gt;\n  distinct(track_id, track_name, artist_name)\n\n# Join with popularity info\nfinal_candidates &lt;- all_candidates |&gt;\n  inner_join(inner_join_data |&gt; select(track_name, artist_name, popularity,valence,energy,danceability), by = c(\"track_name\", \"artist_name\")) |&gt;\n  distinct() |&gt;\n  mutate(\n    is_popular = ifelse(popularity &gt;= 60, \"Not Popular\", \"Popular\"))\npopular_split &lt;- final_candidates |&gt;\n  group_split(is_popular)\n\nnon_popular &lt;- popular_split[[which(levels(as.factor(final_candidates$is_popular)) == \"Not Popular\")]] |&gt;\n  slice_head(n = 4)\n\npopular &lt;- popular_split[[which(levels(as.factor(final_candidates$is_popular)) == \"Popular\")]] |&gt;\n  slice_head(n = 6)\n\nfinal_playlist &lt;- bind_rows(non_popular, popular)\n\n# Select relevant columns from anchor_songs\nanchor_clean &lt;- anchor_songs |&gt;\n  select(track_id, track_name, artist_name, popularity, valence, energy, danceability)\n\n# Combine with the rest of the playlist\nfinal_playlist &lt;- bind_rows(final_playlist, anchor_clean) |&gt;\n  distinct(track_id, .keep_all = TRUE) # Avoid duplicates\n\nkable(final_playlist)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrack_id\ntrack_name\nartist_name\npopularity\nvalence\nenergy\ndanceability\nis_popular\n\n\n\n\n2JdhRRTl3ee6pQQupVQqyb\nHow Great Is Our God\nChris Tomlin\n63\n0.0765\n0.573\n0.318\nNot Popular\n\n\n7oK9VyNzrYvRFo7nQEYkWN\nMr. Brightside\nThe Killers\n78\n0.2320\n0.924\n0.356\nNot Popular\n\n\n5oQcOu1omDykbIPSdSQQNJ\n1985\nBowling For Soup\n70\n0.9030\n0.887\n0.606\nNot Popular\n\n\n7lRlq939cDG4SzWOF4VAnd\nI’m Not Okay (I Promise)\nMy Chemical Romance\n73\n0.2550\n0.940\n0.210\nNot Popular\n\n\n28jOIJzouTp7gqhtx7RbGa\nBlame It On Bad Luck\nBayside\n44\n0.4140\n0.692\n0.323\nPopular\n\n\n4e4vO6bp5nSdP9G79O3qS6\nMiss Me Baby\nChris Cagle\n43\n0.3570\n0.569\n0.526\nPopular\n\n\n2YxoC2dYOotjqt08dEifsc\nChromakey Dreamcoat\nBoards of Canada\n50\n0.7660\n0.825\n0.643\nPopular\n\n\n2itu79WbZhUCHX4jg0fyAd\nNever Let This Go\nParamore\n49\n0.5290\n0.880\n0.519\nPopular\n\n\n53Qpn8LPa0IhFCTiPO0Bbm\nI Disappear\nThe Faint\n45\n0.6770\n0.860\n0.627\nPopular\n\n\n0L639McB94IkUvIgZKM1E5\nPirates\nBullets And Octane\n51\n0.7580\n0.907\n0.521\nPopular\n\n\n3ZffCQKLFLUvYM59XKLbVm\nWake Me Up When September Ends\nGreen Day\n76\n0.1460\n0.814\n0.546\nNA\n\n\n7LVHVU3tWfcxj5aiPFEW4Q\nFix You\nColdplay\n81\n0.1240\n0.417\n0.209\nNA"
  },
  {
    "objectID": "mp03.html#design-principles",
    "href": "mp03.html#design-principles",
    "title": "Mini Project 3",
    "section": "",
    "text": "Using a statistics-based approach, we explored trends in popularity, danceability, energy, valence, track length, and more. By leveraging summary statistics, correlations, and visualizations, we identified key patterns that guided the creation of our desired playlist. The result is a carefully curated selection of tracks that authentically capture the vibe we aimed to achieve.\n\n\n\n\n\n\nBy employing a heuristic-based approach, the selection of songs for the playlist became more precise and intentional. The heuristics used include several key factors: songs that frequently co-occur in playlists alongside anchor tracks, tracks with similar key and tempo, selections from the same artists, songs with acousticness and danceability values within a defined range, and finally, tracks with a comparable valence (emotional tone). By integrating multiple heuristics, this approach effectively curates a playlist where musical and emotional cohesion is maintained, creating a multi-faceted filtering system that enhances meaningful discovery.\n\n\n\n\n\n\nBy prioritizing sonic cohesion, the playlist achieves a well-curated feel. Tracks are ordered based on valence (mood) and energy, ensuring a consistent emotional and rhythmic experience throughout. This approach maintains uniformity in both mood and tempo, enhancing the overall listening journey. Additionally, the visualization of the playlist serves as a validation of its progression, effectively illustrating the evolution of energy, danceability, and emotional tone."
  },
  {
    "objectID": "mp03.html#data-import-and-cleaning",
    "href": "mp03.html#data-import-and-cleaning",
    "title": "Mini Project 3",
    "section": "",
    "text": "For this project, we will examine the key factors that influence a song’s popularity in playlists. Our analysis will utilize the Spotify Million Playlist Dataset, a comprehensive collection of playlists and their associated tracks. Additionally, we will integrate a dataset containing Spotify songs and their audio features to enhance our insights. The table below presents the results of the data import and cleaning process, laying the groundwork for a thorough exploration of song popularity dynamics.\n\n\n\n\nCode\nensure_package &lt;- function(pkg) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n  }\n  library(pkg, character.only = TRUE)\n}\n\n# Ensure the necessary packages are installed and loaded\nensure_package(\"dplyr\")\nensure_package(\"stringr\")\nensure_package(\"tidyr\")\nensure_package(\"httr2\")\nensure_package(\"rvest\")\nensure_package(\"datasets\")\nensure_package(\"purrr\")\nensure_package(\"DT\")\nensure_package(\"jsonlite\")\nensure_package(\"httr\")\nensure_package(\"knitr\")\n\n#Here we will load the song data from a CSV file (posted by Github User gabminamendez) \nload_songs &lt;- function() {\n  # Define file and directory paths\n  directory &lt;- \"data/mp03\"\n  file_url &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n  file_path &lt;- file.path(directory, \"data.csv\")\n  \n  # Create the directory if it doesn't exist\n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n  \n  # Download the file if it doesn't already exist\n  if (!file.exists(file_path)) {\n    download.file(file_url, file_path)\n    message(\"File downloaded: \", file_path)\n  } else {\n    message(\"File already exists: \", file_path)\n  }\n  \n  # Read the CSV file into a data frame\n  song_data &lt;- read.csv(file_path, stringsAsFactors = FALSE)\n  \n  # Process the artists column\n  clean_artist_string &lt;- function(x){\n    str_replace_all(x, \"\\\\['\", \"\") |&gt; \n      str_replace_all(\"'\\\\]\", \"\") |&gt;\n      str_replace_all(\" '\", \"\")\n  }\n  \n  # Process and return the data\n  processed_data &lt;- song_data |&gt; \n    separate_longer_delim(artists, \",\") |&gt;\n    mutate(artist = clean_artist_string(artists)) |&gt;\n    select(-artists)\n  \n  return(processed_data)\n}\n\n# Now actually call the function and save the result\nsong_data &lt;- load_songs()\n\n# View the first few rows to confirm it worked\n# head(song_data)\n# names(song_data)\n# \n# Next, we will load the playlist data from a JSON file (posted by Github User DevinOgrady)\n# Define the function to load the playlist data from the repository, store them locally, and return a tidy data frame to read them into R in a standardized format.\n\nload_playlist &lt;- function() {\n  p_directory &lt;- \"data/mp03/playlists\"\n  p_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/master/data1/\"\n  j_file_name &lt;- \"mpd.slice.0-999.json\"\n  file_path_1 &lt;- file.path(p_directory, j_file_name)\n  file_url_1 &lt;- paste0(p_url, j_file_name)\n  \n  if (!dir.exists(p_directory)) {\n    dir.create(p_directory, recursive = TRUE)\n  }\n  \n  if (!file.exists(file_path_1)) {\n    download.file(file_url_1, destfile = file_path_1, method = \"auto\")\n  }\n  \n  playlist_data &lt;- fromJSON(file_path_1, simplifyVector = FALSE)  \n  \n  if (!\"playlists\" %in% names(playlist_data)) return(NULL)\n  \n  playlist_tracks &lt;- map_dfr(playlist_data$playlists, function(pl) {\n    if (!is.list(pl)) return(NULL)\n    if (!(\"tracks\" %in% names(pl))) return(NULL)\n    if (!is.list(pl$tracks) || length(pl$tracks) == 0) return(NULL)\n    \n    tidy_data &lt;- tryCatch(\n      bind_rows(pl$tracks),\n      error = function(e) NULL\n    )\n    \n    if (is.null(tidy_data) || nrow(tidy_data) == 0) return(NULL)\n    \n    tidy_data |&gt;\n      mutate(\n        playlist_name      = pl$name,\n        playlist_id        = pl$pid,\n        playlist_followers = pl$num_followers,\n        playlist_position  = row_number()\n      ) |&gt;\n      select(\n        playlist_name,\n        playlist_id,\n        playlist_followers,\n        playlist_position,\n        track_name,\n        track_uri,\n        artist_name,\n        artist_uri,\n        album_name,\n        album_uri,\n        duration_ms\n      ) |&gt;\n      rename(\n        track_id  = track_uri,\n        artist_id = artist_uri,\n        album_id  = album_uri,\n        duration  = duration_ms\n      )\n  })\n  \n  return(playlist_tracks)\n}\n\n# Load playlists data\nplaylists_data &lt;- load_playlist()\n# glimpse(playlists_data)\n# head(playlists_data)\n# View(playlists_data)\n\n#‘Rectangle’ the Playlist Data\n#Convert the playlist data into the rectangle format, which is a tidy data frame with one row per track in a playlist.\nstrip_spotify_prefix &lt;- function(x){\n  library(stringr)\n  str_extract(x, \".*:.*:(.*)\", group=1)\n}\n\nplaylists_data &lt;- playlists_data |&gt;\n  mutate(\n    track_id  = sapply(track_id, strip_spotify_prefix),\n    artist_id = sapply(artist_id, strip_spotify_prefix),\n    album_id  = sapply(album_id, strip_spotify_prefix)\n  )\n# View(playlists_data)\n\n#create a kabble of the first 10 rows of the playlist data\nkable(playlists_data[1:10,])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplaylist_name\nplaylist_id\nplaylist_followers\nplaylist_position\ntrack_name\ntrack_id\nartist_name\nartist_id\nalbum_name\nalbum_id\nduration\n\n\n\n\nThrowbacks\n0\n1\n1\nLose Control (feat. Ciara & Fat Man Scoop)\n0UaMYEvWZi0ZqiDOoHU3YI\nMissy Elliott\n2wIVse2owClT7go1WT98tk\nThe Cookbook\n6vV5UrXcfyQD1wu4Qo2I9K\n226863\n\n\nThrowbacks\n0\n1\n2\nToxic\n6I9VzXrHxO9rA9A5euc8Ak\nBritney Spears\n26dSoYclwsYLMAKD3tpOr4\nIn The Zone\n0z7pVBGOD7HCIB7S8eLkLI\n198800\n\n\nThrowbacks\n0\n1\n3\nCrazy In Love\n0WqIKmW4BTrj3eJFmnCKMv\nBeyoncé\n6vWDO969PvNqNYHIOW5v0m\nDangerously In Love (Alben für die Ewigkeit)\n25hVFAxTlDvXbx2X2QkUkE\n235933\n\n\nThrowbacks\n0\n1\n4\nRock Your Body\n1AWQoqb9bSvzTjaLralEkT\nJustin Timberlake\n31TPClRtHm23RisEBtV3X7\nJustified\n6QPkyl04rXwTGlGlcYaRoW\n267266\n\n\nThrowbacks\n0\n1\n5\nIt Wasn’t Me\n1lzr43nnXAijIGYnCT8M8H\nShaggy\n5EvFsr3kj42KNv97ZEnqij\nHot Shot\n6NmFmPX56pcLBOFMhIiKvF\n227600\n\n\nThrowbacks\n0\n1\n6\nYeah!\n0XUfyU2QviPAs6bxSpXYG4\nUsher\n23zg3TcAtWQy7J6upgbUnj\nConfessions\n0vO0b1AvY49CPQyVisJLj0\n250373\n\n\nThrowbacks\n0\n1\n7\nMy Boo\n68vgtRHr7iZHpzGpon6Jlo\nUsher\n23zg3TcAtWQy7J6upgbUnj\nConfessions\n1RM6MGv6bcl6NrAG8PGoZk\n223440\n\n\nThrowbacks\n0\n1\n8\nButtons\n3BxWKCI06eQ5Od8TY2JBeA\nThe Pussycat Dolls\n6wPhSqRtPu1UhRCDX5yaDJ\nPCD\n5x8e8UcCeOgrOzSnDGuPye\n225560\n\n\nThrowbacks\n0\n1\n9\nSay My Name\n7H6ev70Weq6DdpZyyTmUXk\nDestiny’s Child\n1Y8cdNmUJH7yBTd9yOvr5i\nThe Writing’s On The Wall\n283NWqNsCA9GwVHrJk59CG\n271333\n\n\nThrowbacks\n0\n1\n10\nHey Ya! - Radio Mix / Club Mix\n2PpruBYCo4H7WOBJ7Q2EwM\nOutKast\n1G9G7WwrXka3Z1r7aIDjI7\nSpeakerboxxx/The Love Below\n1UsmQ3bpJTyK6ygoOOjG1r\n235213"
  },
  {
    "objectID": "mp03.html#initial-exploration",
    "href": "mp03.html#initial-exploration",
    "title": "Mini Project 3",
    "section": "",
    "text": "With a imported and cleaned dataset, we can now begin to explore the data.\n\n\nHow many distinct tracks and artists are represented in the playlist data?\n\nThere are 9,745 distinct artists and 34,443 distinct tracks in the playlist data.The following table shows the distinct artists in the playlist.\n\n\nCode\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\n#Count the number of distinct artists and tracks\ndistinct_artists &lt;- playlists_data |&gt;\n  distinct(artist_id) |&gt;\n  summarise(\"Number of Artists\" = n())\n#print(distinct_artists)\n\n#create a kable of distinct artists\nkable(distinct_artists)\n\n\n\n\n\nNumber of Artists\n\n\n\n\n9754\n\n\n\n\n\n The following table shows the distinct tracks in the playlist data set.\n\n\nCode\n#Count the number of distinct tracks\ndistinct_tracks &lt;- playlists_data |&gt;\n  distinct(track_id) |&gt;\n  summarise(\"Number of Tracks\" = n())\n#print(distinct_tracks)\n\n#create a kable of distinct tracks\nkable(distinct_tracks)\n\n\n\n\n\nNumber of Tracks\n\n\n\n\n34443\n\n\n\n\n\n\n\nWhat are the 5 most popular tracks in the playlist data?\nThe most popular track is Closer which made 75 appearances in the playlist data.\n\n The following table shows the 5 most popular tracks in the playlist data.\n\n\nCode\n# Find the most popular tracks\nmost_popular_tracks &lt;- playlists_data |&gt;\n  group_by(track_name) |&gt;\n  summarise(`Number of Appearances` = n()) |&gt;\n  arrange(desc(`Number of Appearances`)) |&gt;\n  slice(1:5) |&gt;\n  rename(\"Name of Track\" = track_name) \n#print(most_popular_tracks)\n#View(most_popular_tracks)\n\n# Create a kable of the most popular tracks\nkable(most_popular_tracks)\n\n\n\n\n\nName of Track\nNumber of Appearances\n\n\n\n\nCloser\n75\n\n\nOne Dance\n55\n\n\nHUMBLE.\n52\n\n\nRide\n52\n\n\nBroccoli (feat. Lil Yachty)\n50\n\n\n\n\n\n\n\nWhat is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\nThe most popular track that does not have a corresponding entry in the song characteristics data is One Dance by Drake. It appears 55 times in the playlist data and is shockingly not in the song characteristics data set for it’s danceability.\n\n\n\n\nCode\n#join data frames by track id column\nsong_data &lt;- song_data |&gt;\n  rename(\"track_id\" = id)\n\n#Find the most popular track that has no entry in song  data\nno_match_tracks &lt;- playlists_data |&gt;\n  anti_join(song_data, by = \"track_id\") \n\nmost_pop_nochar &lt;- (\n  no_match_tracks |&gt;\n    count(`Name of Track`= track_name, `Artist` = artist_name, sort = TRUE) |&gt;\n    slice_max(n, n = 1) |&gt;\n    rename(`Number of Appearances` = n)\n)\n\n#View(most_pop_nochar)\n\n# Create a kable of the most popular track without characteristics\nkable(most_pop_nochar)\n\n\n\n\n\nName of Track\nArtist\nNumber of Appearances\n\n\n\n\nOne Dance\nDrake\n55\n\n\n\n\n\n 4. According to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\nThe most danceable track is Tone-Loc's Funky Cold Medina. It appears one time in the playlist titled ***VACATION*** and has a danceability score that is closest to 1.\n\n\nCode\n# use left join to combine the song data with the playlist data\njoin_data &lt;- song_data |&gt;\n  left_join(playlists_data, by = \"track_id\")\n\n# Find the most danceable track\nmost_danceable_track &lt;- join_data |&gt;\n  select(track_name, artist_name, danceability, playlist_name) |&gt;\n  arrange(desc(danceability)) |&gt;\n  rename(\"Name of Track\" = track_name, \"Artist\" = artist_name, \"Danceability\" = danceability, \"Playlist Name\" = playlist_name) |&gt;\n  slice(1)\n\n#View(most_danceable_track)\n\n# Create a kable table to display the most danceable track\nkable(most_danceable_track)\n\n\n\n\n\nName of Track\nArtist\nDanceability\nPlaylist Name\n\n\n\n\nFunky Cold Medina\nTone-Loc\n0.988\nVACATION\n\n\n\n\n\n\n\nWhich playlist has the longest average track length?\nThe playlist with the longest average track length is the Classical playlist. It’s average track length is approximately 411148.7 milliseconds or 7 minutes.\n\n\n\nCode\n# Find the playlist with the longest average track length by grouping by playlist name \n# and calculating the average track length (average of the duration column).\n# Then, calculate the average length in minutes and sort the data frame in descending order.\n#names(playlists_data)\nlongest_playlist &lt;- playlists_data |&gt;\n  group_by(playlist_name) |&gt;\n  summarise(avg_length = mean(duration)) |&gt;\n  mutate(avg_length_min = round(avg_length/60000, 0)) |&gt;\n  arrange(desc(avg_length)) |&gt;\n  rename(\"Playlist Name\" = playlist_name, \"Average Track Length (ms)\" = avg_length, \"Average Track Length (min)\" = avg_length_min) |&gt;\n  slice(1)\n#View(longest_playlist)\n\n# Create a kable Extra table to display the playlist with the longest average track length\nkable(longest_playlist)\n\n\n\n\n\nPlaylist Name\nAverage Track Length (ms)\nAverage Track Length (min)\n\n\n\n\nclassical\n411148.7\n7\n\n\n\n\n\n\n\nWhat is the most popular playlist on Spotify?\nThe most popular playlist on Spotify is Tangled with 1038 followers.\n\n\n\nCode\n#Find the most popular playlist by grouping by playlist id and name and number of followers\nmost_popular_playlist &lt;- playlists_data |&gt;\n  distinct(playlist_id, playlist_name, playlist_followers) |&gt;\n  slice_max(playlist_followers) |&gt;\n  rename(\"Playlist ID\" = playlist_id,\"Playlist Name\" = playlist_name, \"Number of Followers\" = playlist_followers) \n#View(most_popular_playlist)\n\n# Create a kable table to display the most popular playlist\n\nkable(most_popular_playlist)\n\n\n\n\n\nPlaylist ID\nPlaylist Name\nNumber of Followers\n\n\n\n\n765\nTangled\n1038"
  },
  {
    "objectID": "mp03.html#identifying-the-ultimate-playlist",
    "href": "mp03.html#identifying-the-ultimate-playlist",
    "title": "Mini Project 3",
    "section": "",
    "text": "Next, we will conduct a more in-depth analysis of the most popular songs in the data set using INNER_JOIN to combine the song data with the playlist data set. We will use ggplot2 to visualize the data.\n\n\nIs the popularity column correlated with the number of playlist appearances? If so, to what degree?\nThe following plot shows the correlation between the popularity and the number of playlist appearances. The correlation is positive, indicating that as the popularity increases, the number of playlist appearances also increases. The thresholds are set at 0-25 for Low and 25-75 for Medium and 75-100 for High for mean popularity.\n\n\n\n\nCode\n#use inner join to combine the song data with the playlist data\ninner_join_data &lt;- song_data |&gt;\n  inner_join(playlists_data, by = \"track_id\")\n\n#using the inner join data find the correlation between the popularity and the number of playlist appearances\npop_correlation &lt;- inner_join_data |&gt;\n  group_by(track_id, track_name) |&gt;\n  summarise(num_of_playlists = n(),\n            mean_popularity = mean(popularity),\n            .groups = \"drop\") \n\n#create a new column to categorize the popularity into groups (Low, Medium, High)\n\npop_correlation &lt;- pop_correlation |&gt;\n  mutate(popularity_group = cut(\n    mean_popularity,\n    breaks = c(0, 25, 75, 100),\n    labels = c(\"Low\", \"Medium\", \"High\"),\n    include.lowest = TRUE\n  ))\n#View(pop_correlation)\n\n# Create a boxplot to display the popularity groups and their average playlist appearances\nlibrary(ggplot2)\nggplot(pop_correlation, aes(x = popularity_group, y = num_of_playlists)) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"blue\") +\n  labs(\n    title = \"Playlist Appearances by Popularity Group\",\n    x = \"Popularity Group\",\n    y = \"Number of Playlist Appearances\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nIn what year were the most popular songs released?.\nThe most popular songs were released in 2017. The highest average popularity of released songs was approximately 70.79.\n\n\n\nCode\n#Using the inner join data and mean, we will filter the data to find the most popular songs released in a distinct year.\npop_year &lt;- inner_join_data |&gt;\n  group_by(year) |&gt;\n  summarise(mean_pop_songs = mean(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(mean_pop_songs))\n\n#head(pop_year)\ntop_ten_pop_year &lt;- pop_year |&gt;\n  slice_head(n=10)\n#create kable of the most popular songs released in a distinct year\nkable(top_ten_pop_year) \n\n\n\n\n\nyear\nmean_pop_songs\n\n\n\n\n2017\n70.78972\n\n\n1976\n69.37306\n\n\n2016\n67.92846\n\n\n1983\n67.48315\n\n\n1982\n67.05217\n\n\n1980\n66.93662\n\n\n1981\n66.90955\n\n\n1979\n66.62581\n\n\n1967\n66.18493\n\n\n1973\n66.00585\n\n\n\n\n\nCode\n#View(pop_year)\n\n#visualize the average popularity of songs by year\npop_year_plot &lt;- ggplot(pop_year, aes(x = year, y = mean_pop_songs)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"purple\") +\n  labs(title = \" Average Popularity of Released Songs by Year\",\n       x = \"Year\",\n       y = \"Average Popularity\")\nprint(pop_year_plot)\n\n\n\n\n\n\n\n\n\n\n\nIn what year did danceability peak?\nThe year with the highest average danceability is 2017. The average danceability for that year is 0.71.\n\n\n\nCode\n# Using the inner join data, we will filter the data to find the year with the highest average danceability.\ndanceability_year &lt;- inner_join_data |&gt;\n  group_by(year) |&gt;\n  summarise(mean_danceability = mean(danceability, na.rm = TRUE)) |&gt;\n  slice_max(mean_danceability, n = 1) |&gt;\n  rename(\"Average Danceability\" = mean_danceability)\n  \n#View(danceability_year)\n\n#create a kable of the year with the highest average danceability\n\nkable(danceability_year)\n\n\n\n\n\nyear\nAverage Danceability\n\n\n\n\n2017\n0.7120776\n\n\n\n\n\nCode\n#data to view all average danceability by year\ndanceability_year_all &lt;- inner_join_data |&gt;\n  group_by(year) |&gt;\n  summarise(mean_danceability = mean(danceability, na.rm = TRUE)) |&gt;\n  arrange(desc(mean_danceability)) \n\n#visualize the average danceability of songs by year using a line plot\ndanceability_year_plot &lt;- ggplot(danceability_year_all, aes(x = year, y = mean_danceability)) +\n  geom_line(color = \"deepskyblue\") +\n  geom_point(color = \"black\") +\n  labs(title = \"Average Danceability by Year\",\n       x = \"Year\",\n       y = \"Average Danceability\")\nprint(danceability_year_plot)\n\n\n\n\n\n\n\n\n\n\n\nWhich decade is most represented on user playlists?\nThe decade with the most songs is the 2010s. The number of 2010 songs on most users playlist at that time was 24,713.\n\n\n\nCode\n#Mutate a column to calculate the decade using integer division\npop_decade &lt;- inner_join_data |&gt;\n  mutate(decade = (year %/% 10) * 10) |&gt;\n  group_by(decade) |&gt;\n  summarise(num_songs = n()) |&gt;\n  arrange(desc(num_songs)) |&gt;\n  rename(\"Number of Songs\" = num_songs, \"Decade\" = decade)\n\n#View(pop_decade)\n# Create a kable Extra table to display the number of songs by decade\nkable(pop_decade)\n\n\n\n\n\nDecade\nNumber of Songs\n\n\n\n\n2010\n24713\n\n\n2000\n7291\n\n\n1990\n3069\n\n\n1980\n1573\n\n\n1970\n1501\n\n\n1960\n735\n\n\n1950\n93\n\n\n1940\n38\n\n\n1930\n2\n\n\n\n\n\nCode\n# Create a bar plot to visualize the number of songs by decade\npop_decade_plot &lt;- ggplot(pop_decade, aes(x = factor(`Decade`), y = `Number of Songs`)) +\n  geom_bar(stat = \"identity\", fill = \"purple\", color = \"black\") +\n  labs(title = \"Number of Songs by Decade\",\n       x = \"Decade\",\n       y = \"Number of Songs\") +\n  theme_minimal()\n\nprint(pop_decade_plot)\n\n\n\n\n\n\n\n\n\n\n\nPlotting the key frequency among songs\nThe most common key is the 0 and 1 keys. The frequency of those key is more than 1,000.\n\n\n\nCode\n# Calculate the frequency of each key\nkey_frequency &lt;- inner_join_data |&gt;\n  group_by(key) |&gt;\n  summarize(count = n()) |&gt;\n  arrange(desc(count))\n\n# Create a polar plot...\nggplot(key_frequency, aes(x = as.factor(key), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\", color = \"black\") +\n  coord_polar(start = 0) +\n  theme_bw() +\n  labs(title = \"Frequency of Musical Keys\",\n       x = \"Key\",\n       y = \"Count of Keys\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14),\n        plot.title = element_text(size = 16))\n\n\n\n\n\n\n\n\n\n\n\nWhat are the most popular track lengths?\nThe following histogram shows the distribution of track lengths. The most common track length is between 2 and 4 minutes.\n\n\n\nCode\n#using the inner join data, we will filter the data to find the most popular track lengths (in minutes)\ninner_join_data&lt;- inner_join_data |&gt;\n  mutate(track_length = duration/ 60000)\n\n#find the mean, median, min, and max track lengths\ntrack_length_data &lt;- inner_join_data |&gt;\n  summarise(\n    avg_length = mean(track_length, na.rm = TRUE),\n    median_length = median(track_length, na.rm = TRUE),\n    min_length = min(track_length, na.rm = TRUE),\n    max_length = max(track_length, na.rm = TRUE)\n  ) \n#View(track_length_data)\n#Create a kable table to display the track length data\nkable(track_length_data)\n\n\n\n\n\navg_length\nmedian_length\nmin_length\nmax_length\n\n\n\n\n3.974142\n3.833333\n0.0034333\n37.31222\n\n\n\n\n\nCode\n#Create a histogram to visualize the distribution of track lengths\nggplot(inner_join_data, aes(x = track_length)) +\n  geom_histogram(binwidth = 0.6, fill = \"cornflowerblue\", color = \"black\") +\n  labs(title = \"Most Popular Track Lengths\",\n       x = \"Track Length (minutes)\",\n       y = \"Number of Songs\") +\n  theme_bw() +\n  scale_x_continuous(\n    limits = c(0, 10),       # Only min and max go here\n    breaks = seq(0, 10, 2)   # Add ticks every 2 minutes\n  )"
  },
  {
    "objectID": "mp03.html#building-a-playlist-from-anchor-songs",
    "href": "mp03.html#building-a-playlist-from-anchor-songs",
    "title": "Mini Project 3",
    "section": "",
    "text": "Now that exploring the data is finished we can build the playlist! First, we picked two anchor songs from the playlist data set: Wake Me Up When September Ends by Green Day and Fix You by Coldplay. Then, we found songs that work well in the playlist using heuristics.\n\n\n\n\n\nWhat other songs commonly appear on playlists along side this song?\n\n\n\nCode\n#Using inner join data we will filter the data to find the anchor songs and their characteristics. \nanchor_songs &lt;- inner_join_data |&gt;\n  filter(track_name %in% c(\"Wake Me Up When September Ends\", \"Fix You\")) |&gt; \n  distinct(track_id, track_name, artist_name, key, tempo, year, popularity, acousticness, danceability, valence, instrumentalness,energy)\n#View(anchor_songs)\n\n#distinct(inner_join_data, track_name) |&gt; View()\n\n#Heuristic 1: Songs that Commonly Appear with the Anchor Song\n#We'll find songs that appeared in the same playlists as the anchor song(s).\n\nanchor_ids &lt;- anchor_songs$track_id\n\nco_occurring_tracks &lt;- playlists_data |&gt;\n  filter(track_id %in% anchor_ids) |&gt;\n  select(playlist_id) |&gt;\n  inner_join(playlists_data, by = \"playlist_id\") |&gt;\n  filter(!track_id %in% anchor_ids) |&gt;  # exclude the anchor itself\n  count(track_name, artist_name, sort = TRUE) |&gt;\n  slice_max(n, n = 20)\n# Rename columns for clarity\n\nco_occurring_tracks &lt;- co_occurring_tracks |&gt;\n  rename(\n    `Track Name` = track_name,\n    `Artist Name` = artist_name,\n    `Number of Playlists` = n\n  )\n\n#View(co_occurring_tracks)\ntop_ten_co_ocurring &lt;- co_occurring_tracks |&gt;\n  slice_head(n = 10)\n# Create a kable table to display the similar co-occuring tracks\nkable(top_ten_co_ocurring) \n\n\n\n\n\n\n\n\n\n\nTrack Name\nArtist Name\nNumber of Playlists\n\n\n\n\nChasing Cars\nSnow Patrol\n9\n\n\nDrops of Jupiter\nTrain\n8\n\n\nI Write Sins Not Tragedies\nPanic! At The Disco\n7\n\n\nYellow\nColdplay\n7\n\n\n21 Guns\nGreen Day\n6\n\n\nMr. Brightside\nThe Killers\n6\n\n\nShe Will Be Loved - Radio Mix\nMaroon 5\n6\n\n\nThe Scientist\nColdplay\n6\n\n\nHoliday/Boulevard Of Broken Dreams\nGreen Day\n5\n\n\nHow to Save a Life\nThe Fray\n5\n\n\n\n\n\n\n\nWhat other songs are in the same key and have a similar tempo?\n\nThe following table shows the songs that are in the same key and have a similar tempo to the anchor songs. The tempo is between 5 BPM of the anchor songs.\n\n\nCode\n################\n#Heuristic 2: Same Key + Similar Tempo (±5 BPM)\nlibrary(purrr)\nsimilar_key_tempo &lt;- inner_join_data |&gt;\n  filter(\n    key %in% anchor_songs$key,\n    map_lgl(tempo, ~ any(abs(.x - anchor_songs$tempo) &lt;= 5))\n  ) |&gt;\n  distinct(track_id, track_name, artist_name, tempo, key)|&gt;\n  slice_max(tempo, n = 15) # Get top 10 similar tracks\n\n# Rename columns for clarity\nsimilar_key_tempo &lt;- similar_key_tempo |&gt;\n  rename(\n    `Track Name` = track_name,\n    `Artist Name` = artist_name,\n    `Tempo (BPM)` = tempo,\n    `Key` = key\n  )\n\n#View(similar_key_tempo)\ntop_ten_keytemp &lt;- similar_key_tempo  |&gt;\n  slice_head(n = 10)\n# Create a kable table to display the similar key tempo tracks\nkable(top_ten_keytemp) \n\n\n\n\n\n\n\n\n\n\n\n\ntrack_id\nTrack Name\nArtist Name\nTempo (BPM)\nKey\n\n\n\n\n71tjsDvB4EMJqNG8EMmFnb\nComin’ Home Baby\nMel Tormé\n143.121\n7\n\n\n1f2lkuLldqRzvaFjJdjGXX\nSanta Claus Is Comin’ to Town\nMariah Carey\n143.109\n7\n\n\n4NTWZqvfQTlOMitlVn6tew\nThe Show Goes On\nLupe Fiasco\n143.067\n7\n\n\n10eDxSTjwMBq1ZjZK5b1cK\nMillennia\nCrown The Empire\n143.036\n3\n\n\n2rUwQj4SWaP2anuGDtNpYR\nSelf-Made\nBryson Tiller\n143.001\n7\n\n\n0UDCfleTgwihlnOUxbzokR\nAwake\nBTS\n142.816\n3\n\n\n6NGi23FFKq9tH5NR1NcTw2\nLet Her Cry\nHootie & The Blowfish\n142.570\n7\n\n\n3403qFGo7u2ptUyJbdEkjI\nFor the Widows in Paradise, For the Fatherless in Ypsilanti\nSufjan Stevens\n142.433\n3\n\n\n4F55RCGuM477OjznpYGhYz\nShortie Like Mine\nBow Wow\n142.149\n7\n\n\n7g8OpS827dAYU067lZaR0L\nJust a Dream\nCarrie Underwood\n142.060\n7\n\n\n\n\n\n\n\nWhat other songs were released by the same artists?\n\n\n\nCode\n#Heuristic 3: Same Artist\nsame_artist_tracks &lt;- inner_join_data |&gt;\n  filter(artist_name %in% anchor_songs$artist_name) |&gt;\n  distinct(track_id, track_name, artist_name)\n\n#View(same_artist_tracks)\n# Create a kable Extra table to display the same artist tracks\n\ntop_ten_same &lt;- same_artist_tracks |&gt;\n  slice_head(n = 10)\n\nkable(top_ten_same) \n\n\n\n\n\ntrack_id\ntrack_name\nartist_name\n\n\n\n\n31L9yLXSj6LpCFupyMV6CR\nUp&Up\nColdplay\n\n\n5qfZRNjt2TkHEL12r3sDEU\nEverglow\nColdplay\n\n\n4ZcnZVXwLDLWI93SLJER3a\nMisery\nGreen Day\n\n\n1rkbMXhEjIytsUGbhoR5pn\nLife In Technicolor\nColdplay\n\n\n1qIgyDoc2rwtq8w49jeWL8\nOh Love\nGreen Day\n\n\n6f49kbOuQSOsStBpyGvQfA\nA Head Full Of Dreams\nColdplay\n\n\n6FnDerFHdaeCFovZnQ3r14\nLast Ride In\nGreen Day\n\n\n3KzCJGegAcwsSik1bOgkNu\nWords I Might Have Ate\nGreen Day\n\n\n6c6W25YoDGjTq3qSPOga5t\nInk\nColdplay\n\n\n3HWDWyIqWuLsTHECx9DvXF\nBirds\nColdplay\n\n\n\n\n\n\n\nWhat other songs were released in the same year and have similar levels of acousticness and danceability?\n\nWe’ll allow a range (±0.1) around the acousticness/danceability of the anchor songs.\n\n\nCode\n# heuristic 4 - \n# We'll allow a range (±0.1) around the acousticness/danceability of the anchor songs.\n#find the range of years for the anchor songs\n#and filter the data to find the similar songs\n\n\nanchor_range &lt;- anchor_songs |&gt;\n  summarise(\n    min_acoustic = min(acousticness) - 0.1,\n    max_acoustic = max(acousticness) + 0.1,\n    min_dance = min(danceability) - 0.1,\n    max_dance = max(danceability) + 0.1,\n    year_range = list(unique(year))  # make year_range a list-column\n  )\n\n# Pull the scalar values from the 1-row data frame\nmin_acoustic &lt;- anchor_range$min_acoustic\nmax_acoustic &lt;- anchor_range$max_acoustic\nmin_dance &lt;- anchor_range$min_dance\nmax_dance &lt;- anchor_range$max_dance\nyear_range &lt;- anchor_range$year_range[[1]]  # unlist the year_range safely\n\n# Now filter safely\nsimilar_vibe_tracks &lt;- inner_join_data |&gt;\n  filter(\n    year %in% year_range,\n    acousticness &gt;= min_acoustic,\n    acousticness &lt;= max_acoustic,\n    danceability &gt;= min_dance,\n    danceability &lt;= max_dance\n  ) |&gt;\n  distinct(track_id, track_name, artist_name, year)\n\n\n#View(similar_vibe_tracks)\ntop_ten_vibe &lt;- similar_vibe_tracks |&gt;\n  slice_head(n = 10)\n\n#create a kable table to display the similar vibe tracks\nkable(top_ten_vibe)\n\n\n\n\n\n\n\n\n\n\n\ntrack_id\ntrack_name\nartist_name\nyear\n\n\n\n\n28jOIJzouTp7gqhtx7RbGa\nBlame It On Bad Luck\nBayside\n2005\n\n\n4e4vO6bp5nSdP9G79O3qS6\nMiss Me Baby\nChris Cagle\n2005\n\n\n2YxoC2dYOotjqt08dEifsc\nChromakey Dreamcoat\nBoards of Canada\n2005\n\n\n2itu79WbZhUCHX4jg0fyAd\nNever Let This Go\nParamore\n2005\n\n\n53Qpn8LPa0IhFCTiPO0Bbm\nI Disappear\nThe Faint\n2004\n\n\n0L639McB94IkUvIgZKM1E5\nPirates\nBullets And Octane\n2004\n\n\n5fHkJ1a7v0JFPiT9rFQI8U\nTe Daré Lo Mejor\nJesús Adrián Romero\n2004\n\n\n6u84UiQ3TRczfYTDVYrz7S\nSomeone That You’re With\nNickelback\n2005\n\n\n433JymbpWnRMHXzp1oTRP7\nDon’t Bother\nShakira\n2005\n\n\n4sjjV02cN3nDOIZPJabEgM\nThe Time Is Now\nJohn Cena\n2005\n\n\n\n\n\n\n\nWhat other songs were released have similar levels of valence?\n\n\n\nCode\n#Valence(Mood) or emotional tone of the track\n# names(anchor_songs)\n#We will use the valence column to find similar songs\nvalence_range &lt;- anchor_songs |&gt; summarise(\n  min_valence = min(valence) - 0.001,\n  max_valence = max(valence) + 0.001\n)\n#compare  the valence of the anchor songs to the valence of the other songs\nsimilar_valence_tracks &lt;- inner_join_data |&gt;\n  filter(\n    valence &gt;= valence_range$min_valence,\n    valence &lt;= valence_range$max_valence\n  ) |&gt;\n  distinct(track_id, track_name, artist_name, valence)\n\n# Rename columns for clarity\nsimilar_valence_tracks &lt;- similar_valence_tracks |&gt;\n  rename(\n    `Track Name` = track_name,\n    `Artist Name` = artist_name,\n    `Valence` = valence\n  )\n\n#View(similar_valence_tracks)\ntop_ten_valence &lt;- similar_valence_tracks |&gt;\n  slice_head(n = 10)\n# Create a kable table to display the similar valence tracks\nkable(top_ten_valence) \n\n\n\n\n\n\n\n\n\n\n\ntrack_id\nTrack Name\nArtist Name\nValence\n\n\n\n\n3lpa6fzS3rSAbRogEhWxu7\nStar Spangled Banner - Live at Woodstock\nJimi Hendrix\n0.129\n\n\n1YBf7Tq9bpcVwvnlP8YbQS\nYear Zero\nGhost B.C.\n0.137\n\n\n52WTLETEHs5jwCr7LCq0VW\nQueen\nPerfume Genius\n0.130\n\n\n5qfZRNjt2TkHEL12r3sDEU\nEverglow\nColdplay\n0.136\n\n\n0WCbhE2evMrIwRM0DlMy9k\nMixtape (feat. Young Thug & Lil Yachty)\nChance The Rapper\n0.126\n\n\n5eG8QuQKOBvDeCyoOeMkhT\nAlps\nNovo Amor\n0.135\n\n\n7hmdJhc4W0idVVoMES7F9F\nIf I Know Me\nGeorge Strait\n0.146\n\n\n4qKDjmz094Bu2wMepNuwVN\nMain Title / Once Upon A Dream / Prologue - From “Sleeping Beauty” Soundtrack\nChorus - Sleeping Beauty\n0.143\n\n\n1fckqKAI9ug7U1DgQrrOop\nBelieve It - feat. Rick Ross\nMeek Mill\n0.138\n\n\n0st2C7aLz9xkr7JyshhUHK\nScandal of Grace\nHillsong United\n0.138"
  },
  {
    "objectID": "mp03.html#creating-the-ultimate-playlist-1",
    "href": "mp03.html#creating-the-ultimate-playlist-1",
    "title": "Mini Project 3",
    "section": "",
    "text": "The following playlist is comprised of the offical 12 candidates selected for the playlist before ordering. It was created using all four heurtics above to find songs of similar standing as the chosen anchor songs. Additionally, it’s comprised of 2 songs that I was not familiar with, I Disappear and Chromakey Dreamcoat. This means that the playlist is also pretty diverse in terms of popularity.\n\n\n\nCode\n#final Playlist Candidate\n#Now combine all those results and filter down to at least 20 unique songs, making sure at least 8 are not “popular” (say, popularity &lt; 60)\nanchor_artists &lt;- unique(anchor_songs$artist_name)\nanchor_ids &lt;- unique(anchor_songs$track_id)\nall_candidates &lt;- bind_rows(\n  co_occurring_tracks,\n  similar_key_tempo,\n  similar_vibe_tracks,\n  similar_valence_tracks\n) |&gt;\n  filter(!artist_name %in% anchor_artists) |&gt;\n  distinct(track_id, track_name, artist_name)\n\n# Join with popularity info\nfinal_candidates &lt;- all_candidates |&gt;\n  inner_join(inner_join_data |&gt; select(track_name, artist_name, popularity,valence,energy,danceability), by = c(\"track_name\", \"artist_name\")) |&gt;\n  distinct() |&gt;\n  mutate(\n    is_popular = ifelse(popularity &gt;= 60, \"Not Popular\", \"Popular\"))\npopular_split &lt;- final_candidates |&gt;\n  group_split(is_popular)\n\nnon_popular &lt;- popular_split[[which(levels(as.factor(final_candidates$is_popular)) == \"Not Popular\")]] |&gt;\n  slice_head(n = 4)\n\npopular &lt;- popular_split[[which(levels(as.factor(final_candidates$is_popular)) == \"Popular\")]] |&gt;\n  slice_head(n = 6)\n\nfinal_playlist &lt;- bind_rows(non_popular, popular)\n\n# Select relevant columns from anchor_songs\nanchor_clean &lt;- anchor_songs |&gt;\n  select(track_id, track_name, artist_name, popularity, valence, energy, danceability)\n\n# Combine with the rest of the playlist\nfinal_playlist &lt;- bind_rows(final_playlist, anchor_clean) |&gt;\n  distinct(track_id, .keep_all = TRUE) # Avoid duplicates\n\nkable(final_playlist)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrack_id\ntrack_name\nartist_name\npopularity\nvalence\nenergy\ndanceability\nis_popular\n\n\n\n\n2JdhRRTl3ee6pQQupVQqyb\nHow Great Is Our God\nChris Tomlin\n63\n0.0765\n0.573\n0.318\nNot Popular\n\n\n7oK9VyNzrYvRFo7nQEYkWN\nMr. Brightside\nThe Killers\n78\n0.2320\n0.924\n0.356\nNot Popular\n\n\n5oQcOu1omDykbIPSdSQQNJ\n1985\nBowling For Soup\n70\n0.9030\n0.887\n0.606\nNot Popular\n\n\n7lRlq939cDG4SzWOF4VAnd\nI’m Not Okay (I Promise)\nMy Chemical Romance\n73\n0.2550\n0.940\n0.210\nNot Popular\n\n\n28jOIJzouTp7gqhtx7RbGa\nBlame It On Bad Luck\nBayside\n44\n0.4140\n0.692\n0.323\nPopular\n\n\n4e4vO6bp5nSdP9G79O3qS6\nMiss Me Baby\nChris Cagle\n43\n0.3570\n0.569\n0.526\nPopular\n\n\n2YxoC2dYOotjqt08dEifsc\nChromakey Dreamcoat\nBoards of Canada\n50\n0.7660\n0.825\n0.643\nPopular\n\n\n2itu79WbZhUCHX4jg0fyAd\nNever Let This Go\nParamore\n49\n0.5290\n0.880\n0.519\nPopular\n\n\n53Qpn8LPa0IhFCTiPO0Bbm\nI Disappear\nThe Faint\n45\n0.6770\n0.860\n0.627\nPopular\n\n\n0L639McB94IkUvIgZKM1E5\nPirates\nBullets And Octane\n51\n0.7580\n0.907\n0.521\nPopular\n\n\n3ZffCQKLFLUvYM59XKLbVm\nWake Me Up When September Ends\nGreen Day\n76\n0.1460\n0.814\n0.546\nNA\n\n\n7LVHVU3tWfcxj5aiPFEW4Q\nFix You\nColdplay\n81\n0.1240\n0.417\n0.209\nNA"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "",
    "text": "The report analysis below serves to address NYC’s payroll expenses and identify opportunities to spend taxpayer monies more effectively.\nPlease refer to the following dataset:  City Payroll Dataset\nThe goal for this analysis is to analyze the NYC Payroll and report possible savings for the CATS Commissioners consideration\n\n\nThe following table displays NYC Mayor Eric L Adams salary over a 10 year career span in public office. We are required to review this data to understand how NYC taxpayer monies are spent on highly ranked officials like Mr. Adams. As well as, how his salary over the past 10 years compares to the regular NYC employee.\n\n\nCode\nif (!requireNamespace(\"httr2\", quietly = TRUE)) install.packages(\"httr2\")\nif (!requireNamespace(\"jsonlite\", quietly = TRUE)) install.packages(\"jsonlite\")\nif (!requireNamespace(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\nif (!requireNamespace(\"readr\", quietly = TRUE)) install.packages(\"readr\")\nif (!requireNamespace(\"stringr\", quietly = TRUE)) install.packages(\"stringr\")\nif (!requireNamespace(\"DT\", quietly = TRUE)) install.packages(\"DT\")\n\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(DT)\n\nif (file.exists(\"data/mp01/nyc_payroll_export.csv\")) {\n  payroll_data &lt;- read_csv(\"data/mp01/nyc_payroll_export.csv\")\n} else {\n  stop(\"CSV file not found!\")\n}\n\npayroll_data &lt;- read_csv(\"data/mp01/nyc_payroll_export.csv\")\npayroll_data &lt;- payroll_data |&gt; \n  mutate(\n    agency_name = str_to_title(agency_name),\n    last_name  = str_to_title(last_name),\n    first_name = str_to_title(first_name),\n    work_location_borough = str_to_title(work_location_borough),\n    title_description = str_to_title(title_description),\n    leave_status_as_of_june_30 = str_to_title(leave_status_as_of_june_30)\n     ) |&gt;\n  rename(\n    `Fiscal Year`    = fiscal_year,\n    `Agency Name`    = agency_name,\n    `First Name`     = first_name,\n    `Middle Initial` = mid_init,\n    `Last Name`      = last_name,\n    `Position`       = title_description,\n    `Base Salary`    = base_salary,\n    `Start Date`     = agency_start_date,\n    `Reg Hours`      = regular_hours,\n    `Overtime Hours` = ot_hours,\n    `Additional Pay` = total_other_pay,\n    `Payroll Number` = payroll_number,\n    `Work Location`  = work_location_borough,\n    `Leave Status`   = leave_status_as_of_june_30,\n    `Pay Basis`      = pay_basis,\n    `Gross Paid`     = regular_gross_paid,\n    `Overtime Paid`  = total_ot_paid\n  )\n\npayroll_data_filtered &lt;- payroll_data |&gt;\n  filter(`Gross Paid` &gt;= 0) \n\neric_adams_data &lt;- payroll_data_filtered |&gt;\n    filter(`First Name` == \"Eric\", `Middle Initial` == \"L\", `Last Name` == \"Adams\") |&gt;\n    mutate(`Total Salary` = `Gross Paid` + `Overtime Paid` + `Additional Pay`) |&gt;\n  group_by(`Fiscal Year`) \n\neric_adams_career &lt;- eric_adams_data |&gt;\n  group_by(`Fiscal Year`, `Agency Name`) |&gt;\n  summarise(\n    `Total Salary` = sum(`Total Salary`, na.rm = TRUE),\n    `Position` = paste(unique(`Position`), collapse = \",\"),\n    .groups = \"drop\"\n    ) \n\n\n\n\n\n\n\n\n\n\nThe Mayor’s salary has consistently increased since he became Borough President of Brooklyn. As we analyze the data set, we will see that the adjustment of Mayor’s salary every year is differs from non high ranking employees and high ranking officials in NYC’s payroll.\n\n\n\n\nThe following findings1 below serve to help identify factors to consider as we conduct analysis for the purposes of recommending the best policy for adoption by the CATS Commissioners.\n\n\n\n\n\n\n\n\nThe position with the highest base rate of pay is the Chief Actuary who has a per Annum salary of $296,470.38. The fixed salary of the Chief Actuary is comparable to the Mayor and other high ranking officials.\n\n\n\n\n\n\n\n\n\nThe government employee with the single highest total payroll is Chief Marine Engineer, Pavel Kotelevich. Pavel’s total payroll is $697,052.01. This amount includes his base salary per annum and overtime wages paid. Since Pavel had over 1000 hours of overtime hours, he has received substantial overtime wages despite making a fixed salary of 169,520 dollars in fiscal year 2024.\n\n\n\n\n\n\n\n\n\nThe Department of Correction has the most overtime hours recorded compared to any other agency. The results show that James Inernicola has one of the highest overtime hours recorded, 3692.90 hours. We can assume that the Department of Correction allocates a part of it’s budget to overtime wages for the police force. Therefore, the city’s payroll must also allocate a part of it’s budget for overtime expenses in large department’s like the Department of Correction.\n\n\n\n\n\n\n\n\n\nThe Districting Commission has on record one of the highest average annual payroll recorded at $158,699.67 in 2014. This result conveys how the agency allocates this amount each year in their budget to pay each employee wages.\n\n\n\n\n\n\n\n\n\nThe Department of ED Pedagogical has the highest recorded number of employees on payroll of over 100,000 employees employed per year. The city payroll must allocate a part of it’s budget to the wages and hiring expenses of this agency and other agencies.\n\n\n\n\n\n\n\n\n\nThe Fire Department has the largest average overtime usage compared to the average regular hours recorded at 346.40 hours. Similar to the Department of Correction, the Fire Department has to allocate a part of it’s budget to overtime wages of it’s firefighter workforce.\n\n\n\n\n\n\n\n\n\nThe average salary of employees that work outside of the 5 NYC boroughs is little over $100,000.00. This means that the taxpayer monies that fund these jobs are not being spent in the NYC market or economy.\n\n\n\n\n\n\n\n\n\nThe growth rate from 2014 to 2024 is approximately 46.86%. This is the percentage increase in the total payroll from 2014 to 2024, a 10 year span. This means that NYC’s aggregate wage increases are on track to meet 50% in the coming years"
  },
  {
    "objectID": "mp01.html#policy-1---capping-salaries-at-mayoral-level",
    "href": "mp01.html#policy-1---capping-salaries-at-mayoral-level",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "Policy 1 - Capping Salaries at Mayoral Level",
    "text": "Policy 1 - Capping Salaries at Mayoral Level\nThe data set contains information on the different tiers of income for employees (i.e. per annum, per hour, etc.) The main group of employee with a fixed salary (per Annum) are high ranking officials including the Mayor Eric L Adams. The following findings showcase the results of capping all employee wages at the Mayoral Level.\n\nTotal Mayoral Pay For each Fiscal Year\nThe following table displays the salary of the Mayor Eric L Adams from 2014 to 2024.\n\n\n\n\n\n\n\n\nTotal Payroll of Employees Paid More than the Mayor\n\n\n\n\n\n\nBased on the data displayed in the table, there are over 90 employees who make more than the Mayor Eric L Adams each fiscal year. This table allows us to conclude that fixed salaries are not based on title in the NYC government. Therefore, capping the salary at the mayoral level may be feasible given that the position holds no weight in payroll.\n\n\nTotal Savings if the Mayor’s Salary is the Limit\n\n\n\n\n\n\nAccording to the table above, the total savings generated each year by capping employee salary based on the Mayor’s Salary does not exceed $200,000.\n\n\nAgencies and positions that would be affected the most by this policy\nThe following table displays only the top earning employees of each fiscal year who would be affected.\n\n\n\n\n\n\nAccording to this table above, over 90 employees will be affected by the adoption of this policy.2 The Commission may need to consider the affects this would have on the employees who believe their salary is correct for the workload and position they have.\n\n\nRecommendation to the CATS commissioners on whether this policy should be adopted.\nBased on findings above, we do not recommend the CATS Commissioners to adopt this policy because the total savings generated by capping employee salary based on the Mayor’s Salary does not exceed $200,000 regardless of the year. In addition, the city would lose 102,793.00 if this policy was enacted in 2022. This policy would affected a large part of the city workforce, which may cause turnover rates to skyrocket."
  },
  {
    "objectID": "mp01.html#policy-2---increasing-staffing-to-reduce-overtime",
    "href": "mp01.html#policy-2---increasing-staffing-to-reduce-overtime",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "Policy 2 - Increasing Staffing to Reduce Overtime",
    "text": "Policy 2 - Increasing Staffing to Reduce Overtime\nThe data set showcases how overtime accounts for a large portion of city payroll. The following analyzes the potential upside of increasing staffing to reduce the effects of overtime expenses.\n\nTotal Overtime Hours and Pay by Agency and Job Title\n\n\n\n\n\n\nAccording to this table, there are over 27,000 positions where overtime hours are logged in each agency every fiscal year. The agencies may benefit from a reduction in the workforce due to the millions spent every year to fund the overtime of these positions.\n\n\nCorrelation between increase in employees hired and overtime hours\n\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\n\n\n\n\n\n\n\nThe correlation between employee count and overtime hours is 0.6179. Therefore, as employee hiring increases the amount of overtime hours will also increase. This suggests that hiring more employees could increase overtime expenses.\n\n\nRecommendation to the CATS commissioners on whether this policy should be adopted.\nBased on the findings above, I recommend the CATS commissioner to adopt this policy with caution. I support this policy due to the millions that would be saved over time from the increase in staff."
  },
  {
    "objectID": "mp01.html#policy-3-my-policy-proposal---combining-a-decrease-in-hourly-employees-and-policy-2",
    "href": "mp01.html#policy-3-my-policy-proposal---combining-a-decrease-in-hourly-employees-and-policy-2",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "Policy 3: My Policy Proposal - Combining A Decrease in Hourly Employees and Policy 2",
    "text": "Policy 3: My Policy Proposal - Combining A Decrease in Hourly Employees and Policy 2\n\nConclusion based on full analysis of dataset:\nBased on my analysis of the data set, the City of New York spends most of it’s money on overtime expenses of the employees. The City should hire more employees to reduce the overtime hours, but also decrease the amount of hourly employees they hire. This has two advantages for the City Payroll. First, it will allow the workload to be spread out amount the full time employees with a fixed per Annual salary. Second, the City will save money in the long term because it will not have a large part time workforce to pay overtime wages.\n\nTable of Total Savings from Policy 3\nThe following table generates total savings (per Agency and Per fiscal year) from the reduction of overtime expenses and from the reduction in the number of hourly employees.\n\n\n\n\n\n\n\n\n\nRecommendation to the CATS commissioners on whether this policy should be adopted.\nFrom the table above, I can conclude that millions of dollars would be saved in each agency per fiscal year if this policy was adopted.\n\nLast Updated: Wednesday 04 23, 2025 at 22:09PM"
  },
  {
    "objectID": "mp01.html#footnotes",
    "href": "mp01.html#footnotes",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlease note the following does not include data from the columns regular_gross_paid, total_ot_paid and total_other_paid↩︎\nIn the full length display close to 1000 employees would be affected by this salary cap.↩︎"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "A Glimpse Of Stephanie",
    "section": "About Me!",
    "text": "About Me!\n\n\n\n\n\nflowchart TD\n  A(Stephanie) --&gt; B(Born in Brooklyn, NY)\n  B --&gt; C{Education}\n  C --&gt; D(Bachelor's of Finance)\n  C --&gt; E(Master's of Science in Business Analytics w/ Concentration in Data Analytics)\n\n\n\n\n\n\nHi! I’m Stephanie. I’m a New York native who has lived in two of NYC’s boroughs. I currently live in Nassau County, Long Island. I graduated from Muhlenberg College with a Bachelor of Arts in Finance. I am pursuing a master’s degree in business analytics because I would like to become proficient in data analysis and software tools. In addition, I am invested in learning how to use data to craft compelling stories that influence insights and decision-making.\n\nFun Facts\n\nI used to work for TD Bank\nI have a Series 7 License\nI almost became an accountant\n\n\n\n\n\nPhoto by Annie Spratt\n\n\n\n\nLast Updated: Wednesday 04 23, 2025 at 22:07PM"
  },
  {
    "objectID": "index1.html#about-me",
    "href": "index1.html#about-me",
    "title": "A Glimpse Of Stephanie",
    "section": "About Me!",
    "text": "About Me!\n\n\n\n\n\nflowchart TD\n  A(Stephanie) --&gt; B(Born in Brooklyn, NY)\n  B --&gt; C{Education}\n  C --&gt; D(Bachelor's of Finance)\n  C --&gt; E(Master's of Science in Business Analytics w/ Concentration in Data Analytics)\n\n\n\n\n\n\nHi! I’m Stephanie. I’m a New York native who has lived in two of NYC’s boroughs. I currently live in Nassau County, Long Island. I graduated from Muhlenberg College with a Bachelor of Arts in Finance. I am pursuing a master’s degree in business analytics because I would like to become proficient in data analysis and software tools. In addition, I am invested in learning how to use data to craft compelling stories that influence insights and decision-making.\n\nFun Facts\n\nI used to work for TD Bank\nI have a Series 7 License\nI almost became an accountant\n\n\n\nLast Updated: Wednesday 04 23, 2025 at 22:07PM"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Green Transit Alliance for Investigation of Variance (GTA IV) ‘Greenest’ Awards",
    "section": "",
    "text": "The Green Transit Alliance for Investigation of Variance (GTA IV) is excited to announce the three GTA IV Green Transit Awards winners for the following awards: Greenest Transit Agency, Agency with the Highest Percentage of Electrification, Worst Agency by Highest Total Emissions. We reserved a honorable mention for the Best Small, Medium and Large Agencies at the end.\n\n\n\nGreenest Transit Agency:\n\nThe Greenest Transit Agency award goes to the agency with the lowest emissions per passenger mile. The winner of this award is the “City Of Portland” with an average of 0 pounds of CO2 per passenger mile. This agency has demonstrated a commitment to reducing its carbon footprint and promoting sustainable transportation practices. \n\n\nAgency with the Highest Percentage of Electrification:\n\nThe Agency with the Highest Percentage of Electrification award goes to the agency with the highest percentage of electric propulsion vehicles in its fleet. The winner of this award is the “City Of Portland” with 100% of its fleet using electric propulsion. This agency has shown a strong commitment to reducing emissions and transitioning to cleaner energy sources. \n\n\nWorst Agency based on Highest Total Emissions\n\nThe Worst Agency award goes to the agency with the highest total emissions. The winner of this award is the “New Jersey Transit Corporation” with a total of 875972108 pounds of CO2 emitted. This agency has the highest total emissions, indicating a need for improvement in reducing its carbon footprint and promoting sustainable transportation practices.\n\n\n\nData Import and Data Cleaning\n The data was imported from various sources, including the Energy Information Administration (EIA) and the National Transit Database (NTD). The data was cleaned and processed to calculate emissions, normalize emissions to transit usage, and categorize emissions based on specific criteria. The data was then analyzed to identify the winners of the GTA IV Green Transit Awards.\nThe data code is hidden for readability. To view the code, click on the “Show Code” button below each section.\n\n\nCode\n#The following Code loads the required libraries and packages\nensure_package &lt;- function(pkg){\n  pkg &lt;- as.character(substitute(pkg))\n  options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n  if(!require(pkg, character.only=TRUE)) install.packages(pkg)\n  stopifnot(require(pkg, character.only=TRUE))\n}\n\nensure_package(dplyr)\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\nensure_package(stringr)\n\n#The following code loads the required data\nget_eia_sep &lt;- function(state, abbr){\n  state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n  \n  dir_name &lt;- file.path(\"data\", \"mp02\")\n  file_name &lt;- file.path(dir_name, state_formatted)\n  \n  dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n  \n  if(!file.exists(file_name)){\n    BASE_URL &lt;- \"https://www.eia.gov\"\n    REQUEST &lt;- request(BASE_URL) |&gt; \n      req_url_path(\"electricity\", \"state\", state_formatted)\n    \n    RESPONSE &lt;- req_perform(REQUEST)\n    \n    resp_check_status(RESPONSE)\n    \n    writeLines(resp_body_string(RESPONSE), file_name)\n  }\n  \n  TABLE &lt;- read_html(file_name) |&gt; \n    html_element(\"table\") |&gt; \n    html_table() |&gt;\n    mutate(Item = str_to_lower(Item))\n  \n  if(\"U.S. rank\" %in% colnames(TABLE)){\n    TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n  }\n  \n  CO2_MWh &lt;- TABLE |&gt; \n    filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n    pull(Value) |&gt; \n    str_replace_all(\",\", \"\") |&gt;\n    as.numeric()\n  \n  PRIMARY &lt;- TABLE |&gt; \n    filter(Item == \"primary energy source\") |&gt; \n    pull(Rank)\n  \n  RATE &lt;- TABLE |&gt;\n    filter(Item == \"average retail price (cents/kwh)\") |&gt;\n    pull(Value) |&gt;\n    as.numeric()\n  \n  GENERATION_MWh &lt;- TABLE |&gt;\n    filter(Item == \"net generation (megawatthours)\") |&gt;\n    pull(Value) |&gt;\n    str_replace_all(\",\", \"\") |&gt;\n    as.numeric()\n  \n  data.frame(CO2_MWh               = CO2_MWh, \n             primary_source        = PRIMARY,\n             electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n             # * 1000 kWh to MWH \n             generation_MWh        = GENERATION_MWh, \n             state                 = state, \n             abbreviation          = abbr\n  )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\n\n ##### The datable below presents the data used to determine the winners of the GTA IV Green Transit Awards. The data includes information on the pounds of CO2 emitted per MWh of electricity produced, the primary source of electricity generation, the average retail price for 1000 kWh, and the total generation capacity in MWh for each state. The data has been cleaned and formatted for analysis. \n\n\nCode\nensure_package(scales)\nensure_package(DT)\n\n#The following code cleans the data and answers the questions\nEIA_SEP_REPORT &lt;- EIA_SEP_REPORT |&gt; \n  select(-abbreviation) |&gt;\n  arrange(desc(CO2_MWh)) |&gt;\n  mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n         electricity_price_MWh = dollar(electricity_price_MWh), \n         generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n  rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n         `Primary Source of Electricity Generation`=primary_source, \n         `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n         `Total Generation Capacity (MWh)`= generation_MWh, \n         State=state)\n# Display the table with formatting and caption\ndatatable(EIA_SEP_REPORT, caption = htmltools::tags$caption(\n  style = \"font-size: 18px;\", \"Table 1: State Electricity Profiles\"),\n  options = list( autoWidth = TRUE),\n  rownames = FALSE) |&gt;\n  formatStyle(columns = \"Pounds of CO2 Emitted per MWh of Electricity Produced\", \n              backgroundColor = \"#f2f2f2\", \n              fontWeight = 'bold') |&gt;\n  formatStyle(columns = \"Average Retail Price for 1000 kWh\", \n              backgroundColor = \"#f2f2f2\", \n              fontWeight = 'bold') |&gt;\n  formatStyle(columns = \"Total Generation Capacity (MWh)\", \n              backgroundColor = \"#f2f2f2\", \n              fontWeight = 'bold') |&gt;\n  formatStyle(columns = \"Primary Source of Electricity Generation\", \n              backgroundColor = \"#f2f2f2\", \n              fontWeight = 'bold') |&gt;\n  formatStyle(columns = \"State\",\n              backgroundColor = \"#f2f2f2\", \n              fontWeight = 'bold')\n\n\n\n\n\n\n ### Initial Analysis of SEP Data  The following data tables provide insights into analysis of the SEP Data. The tables answer the following questions: \n\n\n1. Which state has the most expensive retail electricity?\nThe state with the most expensive retail electricity is Hawaii with an average retail price of $33.01 for 1000 kWh or $386. \n\n\nCode\n#Question 1:\n#Which state has the most expensive retail electricity?\nEIA_SEP_REPORT_TOP_STATE &lt;- EIA_SEP_REPORT |&gt; \n  select(State,`Average Retail Price for 1000 kWh` ) |&gt;\n  mutate(`Average Retail Price for 1000 kWh` = as.numeric(str_replace_all(`Average Retail Price for 1000 kWh`, \"[^0-9\\\\.]\", \"\"))) |&gt;\n  arrange(desc(`Average Retail Price for 1000 kWh`))|&gt; \n  slice(1)\n\nEIA_SEP_REPORT_TOP_STATE &lt;- EIA_SEP_REPORT_TOP_STATE |&gt; \n  mutate(`Average Retail Price for 1000 kWh` = dollar(`Average Retail Price for 1000 kWh`))\n\n# Display the table with formatting and caption\ndatatable(EIA_SEP_REPORT_TOP_STATE, caption = htmltools::tags$caption(\n  style = \"font-size: 18px;\", \"Table 2: State with the most expensive retail electricity\"), rownames =   FALSE,\n  options = list(dom = 't', paging = FALSE, searching = FALSE, ordering = FALSE)) |&gt;\n  formatStyle(columns = \"Average Retail Price for 1000 kWh\", \n              backgroundColor = \"#FFCCCB\", \n              fontWeight = 'bold')\n\n\n\n\n\n\n ### 2. Which state has the ‘dirtiest’ electricity mix?\nThe state with the ‘dirtiest’ electricity mix is West Virginia with 1,925 pounds of CO2 emitted per MWh of electricity produced. \n\n\nCode\n#Question 2:\n#Which state has the 'dirtiest' electricity mix?\n# Step 1: Process and clean the data first, and assign it to the EIA_SEP_REPORT_DIRTIEST variable\nEIA_SEP_REPORT_DIRTIEST &lt;- EIA_SEP_REPORT |&gt; \n  filter(!is.na(`Pounds of CO2 Emitted per MWh of Electricity Produced`)) |&gt;  # Remove NAs\n  select(State, `Pounds of CO2 Emitted per MWh of Electricity Produced`) |&gt;\n  # Clean the column, remove any non-numeric characters, and convert to numeric\n  mutate(`Pounds of CO2 Emitted per MWh of Electricity Produced` = \n           as.numeric(str_replace_all(`Pounds of CO2 Emitted per MWh of Electricity Produced`, \"[^0-9\\\\.]\", \"\"))) |&gt;\n  arrange(desc(`Pounds of CO2 Emitted per MWh of Electricity Produced`)) |&gt;  # Sort in descending order\n  slice(1) |&gt;  # Select the state with the highest emissions\n  # Apply formatting after ensuring numeric type\n  mutate(`Pounds of CO2 Emitted per MWh of Electricity Produced` = \n           number(`Pounds of CO2 Emitted per MWh of Electricity Produced`, big.mark=\",\"))\n\n# Display the table with formatting and caption\ndatatable(EIA_SEP_REPORT_DIRTIEST, \n          caption = htmltools::tags$caption(\n            style = \"font-size: 16px;\", \"Table 3: State with the dirtiest electricity mix\"), rownames = FALSE,\n          options = list(dom = 't', paging = FALSE, searching = FALSE, ordering = FALSE)) |&gt;\n  formatStyle(columns = \"Pounds of CO2 Emitted per MWh of Electricity Produced\", \n              backgroundColor = \"#FFCCCB\", \n              fontWeight = 'bold')\n\n\n\n\n\n\n\n\n\n3. On average, how many pounds of CO2 are emitted per MWh of electricity produced in the US?\nThe weighted average of CO2 that are emitted per MWh of electricity produced in the US is 805. \n\n\nCode\n#Question 3: \n#On average, how many pounds of CO2 are emitted per MWh of electricity produced in the US? (Note that you will need to use a suitably weighted average here.)\nEIA_SEP_REPORT_WEIGHTED_AVG &lt;- EIA_SEP_REPORT |&gt; \n  filter(!is.na(`Pounds of CO2 Emitted per MWh of Electricity Produced`)) |&gt;  # Remove NAs\n  select(`Pounds of CO2 Emitted per MWh of Electricity Produced`, `Total Generation Capacity (MWh)`) |&gt;\n  mutate(`Pounds of CO2 Emitted per MWh of Electricity Produced` = \n           as.numeric(str_replace_all(`Pounds of CO2 Emitted per MWh of Electricity Produced`, \"[^0-9\\\\.]\", \"\"))) |&gt;\n  mutate(`Total Generation Capacity (MWh)` =\n           as.numeric(str_replace_all(`Total Generation Capacity (MWh)`, \"[^0-9\\\\.]\", \"\"))) |&gt;\n  # Calculate the weighted CO2 emissions\n  mutate(weighted_CO2 = `Pounds of CO2 Emitted per MWh of Electricity Produced` * `Total Generation Capacity (MWh)`) |&gt;\n  summarise(weighted_avg_CO2 = sum(weighted_CO2) / sum(`Total Generation Capacity (MWh)`)) |&gt;\n  mutate(weighted_avg_CO2 = number(weighted_avg_CO2, big.mark=\",\"))\n\n#Display the result the weighted average CO2 emissions per MWh of electricity produced in the US\ndatatable(EIA_SEP_REPORT_WEIGHTED_AVG, \n          caption = htmltools::tags$caption(\n            style = \"font-size: 18px;\", \"Weighted Average of CO2 Emissions per MWh of Electricity Produced accross all states in the US\") ,rownames = FALSE,\n          options = list(dom = 't', paging = FALSE, searching = FALSE, ordering = FALSE)) |&gt;\n  formatStyle(columns = \"weighted_avg_CO2\", \n              backgroundColor = \"#FFCCCB\", \n              fontWeight = 'bold')\n\n\n\n\n\n\n ### 4. What is the rarest primary energy source in the US? What is the associated cost of electricity and where is it used?\nThe rarest primary energy source in the US is Petroleun with an average retail price of 386 for 1000 kWh. It is most commonly used in Hawaii. \n\n\nCode\n#Question 4\n#What is the rarest primary energy source in the US? What is the associated cost of electricity and where is it used?\nEIA_SEP_REPORT_RAREST_ENERGY_SOURCE_STATE &lt;- EIA_SEP_REPORT |&gt;\n  select(State, `Primary Source of Electricity Generation`, `Average Retail Price for 1000 kWh`) |&gt;\n  group_by(`Primary Source of Electricity Generation`) |&gt;\n  summarise(\n    count = n(),\n    `Average Retail Price for 1000 kWh` = mean(as.numeric(str_replace_all(`Average Retail Price for 1000 kWh`, \"[^0-9\\\\.]\", \"\")), na.rm = TRUE),\n    State = names(sort(table(State), decreasing = TRUE))[1]  # Get the state with the most occurrences\n  ) |&gt;\n  arrange(count) |&gt;\n  slice(1)  # This will give you the rarest energy source with the average price and most common state\nEIA_SEP_REPORT_RAREST_ENERGY_SOURCE_STATE &lt;- EIA_SEP_REPORT_RAREST_ENERGY_SOURCE_STATE |&gt;\n  mutate(`Average Retail Price for 1000 kWh` = dollar(`Average Retail Price for 1000 kWh`)) |&gt;\n  select(-count)  # Remove the count column\n# Display result as a datatable\ndatatable(EIA_SEP_REPORT_RAREST_ENERGY_SOURCE_STATE, \n          caption = htmltools::tags$caption(\n            style = \"font-size: 18px;\", \n            \"Table 4: Rarest Primary Energy Source with Average Retail Price and Most Common State\"\n          ), rownames = FALSE,\n          options = list(dom = 't', paging = FALSE, searching = FALSE, ordering = FALSE)) |&gt;\n  formatStyle(columns = c(\"Average Retail Price for 1000 kWh\"), \n              backgroundColor = \"#f2f2f2\", \n              fontWeight = 'bold')\n\n\n\n\n\n\n ### 5. Compare NY’s energy mix than that of Texas?\nNew York’s energy mix is 1.5 times cleaner than that of Texas. \n\n\nCode\n#Question 5:\n#How many times cleaner is NY’s energy mix than that of Texas?\nEIA_SEP_REPORT_CLEANER_MIX &lt;- EIA_SEP_REPORT |&gt;\n  select(State, `Pounds of CO2 Emitted per MWh of Electricity Produced`) |&gt;\n  filter(State %in% c(\"New York\", \"Texas\")) |&gt;\n  mutate(`Pounds of CO2 Emitted per MWh of Electricity Produced` = as.numeric(str_replace_all(`Pounds of CO2 Emitted per MWh of Electricity Produced`, \"[^0-9\\\\.]\", \"\"))) |&gt;\n  arrange(State) |&gt;  # Arrange the data by State for clarity\n  group_by(State) |&gt;  # Group by state to compute values for each one separately\n  summarise(\n    `Pounds of CO2 Emitted per MWh of Electricity Produced` = mean(`Pounds of CO2 Emitted per MWh of Electricity Produced`, na.rm = TRUE),\n    .groups = 'drop'\n  ) |&gt;  # Ensure no lingering grouping after summarising\n  mutate(ratio = `Pounds of CO2 Emitted per MWh of Electricity Produced`[State == \"New York\"] /\n           `Pounds of CO2 Emitted per MWh of Electricity Produced`[State == \"Texas\"]) |&gt;\n  mutate(ratio = round(ratio, 2))  \n\n# Display the result as a datatable\n\ndatatable(EIA_SEP_REPORT_CLEANER_MIX, rownames = FALSE, caption = htmltools::tags$caption(\n  style = \"font-size: 18px;\",\"Table 5: New York's energy mix compared to Texas's energy mix (by cleanliest)\"),\n  options = list(dom = 't', paging = FALSE, searching = FALSE, ordering = FALSE)) |&gt;\n  formatStyle(columns = \"State\",  # Format based on the \"State\" column\n              target = 'row',  # Apply style to entire row\n              backgroundColor = styleEqual(c(\"New York\", \"Texas\"), c(\"#f2f2f2\", \"#DCDCDC\")),\n              fontWeight = 'bold')\n\n\n\n\n\n\n\n\nData Import For the National Transit Database (NTD) Energy Data\n The following code imports the National Transit Database (NTD) data for analysis. The data includes information on energy consumption, service, and transit modes for various agencies. The data is cleaned and processed to calculate emissions, normalize emissions to transit usage, and categorize emissions based on specific criteria. The data is then analyzed to identify the winners of the GTA IV Green Transit Awards.\nThe datatable in the following code displays the unique modes of transportation in the NTD Energy data.\n\n\nCode\n#Data Import Part 2\n#Annual Database Energy Consumption 2023  \nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n  DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                      destfile=NTD_ENERGY_FILE, \n                      method=\"curl\")\n  \n  if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n    cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x){\n  x &lt;- if_else(x == \"-\", NA, x)\n  replace_na(as.numeric(x), 0)\n}\n#Clean up data\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n  select(-c(`Reporter Type`, \n            `Reporting Module`, \n            `Other Fuel`, \n            `Other Fuel Description`)) |&gt;\n  mutate(across(-c(`Agency Name`, \n                   `Mode`,\n                   `TOS`), \n                to_numeric_fill_0)) |&gt;\n  group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n  summarize(across(where(is.numeric), sum), \n            .groups = \"keep\") |&gt;\n  mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n  filter(ENERGY &gt; 0) |&gt;\n  select(-ENERGY) |&gt;\n  ungroup()\n\n# Display 10 random rows\nslice_sample(NTD_ENERGY , n=10)\n\n\n# A tibble: 10 × 16\n   `NTD ID` Mode  `Agency Name`       `Bio-Diesel` `Bunker Fuel` `C Natural Gas`\n      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                      &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n 1    40037 DR    Board of County Co…            0             0               0\n 2    40185 DR    Bay County Transpo…            0             0               0\n 3    80007 DR    City of Pueblo                 0             0               0\n 4    50066 HR    Chicago Transit Au…            0             0               0\n 5    90014 MB    Alameda-Contra Cos…            0             0               0\n 6    60108 MB    Harris County                  0             0               0\n 7    10087 MB    City of Nashua             14416             0           65272\n 8    10066 DR    Green Mountain Tra…            0             0               0\n 9    50010 DR    METRO Regional Tra…            0             0               0\n10    90001 CB    Regional Transport…         3590             0               0\n# ℹ 10 more variables: `Diesel Fuel` &lt;dbl&gt;, `Electric Battery` &lt;dbl&gt;,\n#   `Electric Propulsion` &lt;dbl&gt;, Ethanol &lt;dbl&gt;, Methonal &lt;dbl&gt;, Gasoline &lt;dbl&gt;,\n#   Hydrogen &lt;dbl&gt;, Kerosene &lt;dbl&gt;, `Liquified Nat Gas` &lt;dbl&gt;,\n#   `Liquified Petroleum Gas` &lt;dbl&gt;\n\n\nCode\nunique(NTD_ENERGY$Mode)\n\n\n [1] \"DR\" \"FB\" \"MB\" \"SR\" \"TB\" \"VP\" \"CB\" \"RB\" \"LR\" \"MG\" \"CR\" \"AR\" \"TR\" \"HR\" \"YR\"\n[16] \"IP\" \"PB\" \"CC\"\n\n\nCode\n# Get unique Mode codes from the data\nunique_modes &lt;- NTD_ENERGY |&gt;\n  distinct(Mode)  # This will return only the unique Mode values\n\n# Display the unique Mode codes\n#print(unique_modes)\ndatatable(unique_modes, caption = \"Unique Modes in NTD_Energy\")\n\n\n\n\n\n\n ### Recoding the Mode Column\nThe following code recodes the Mode column in the NTD Energy data to change the names of the unique modes to more descriptive names. This recoding will make the data more readable and easier to interpret for analysis.\n\n\nCode\n#Recoding the Mode Column - changing the names of the unique modes in the Mode column \nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n  mutate(Mode=case_when(\n    Mode == \"DR\"  ~ \"Demand Response\",  \n    Mode == \"FB\"  ~ \"Ferryboat\",\n    Mode == \"MB\"  ~ \"Motorbus\" ,\n    Mode == \"SR\"  ~ \"Street Car Rail\",\n    Mode == \"TB\"  ~ \"Trolleybus\",\n    Mode == \"VP\"  ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Rapid Bus\" , \n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MG\" ~ \"Monorail Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",  \n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",  \n    Mode == \"IP\" ~ \"Inclined Plane Vehicle\", \n    Mode == \"PB\" ~ \"Publico Mode\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))\n\n\n ### Data Import for the NTD Service Data\nThe following code imports the National Transit Database (NTD) Service data for analysis. The data includes information on transit service, unlinked passenger trips, and passenger miles for various agencies. The data is cleaned and processed to calculate emissions, normalize emissions to transit usage, and categorize emissions based on specific criteria. The data is then analyzed to identify the winners of the GTA IV Green Transit Awards.\n\n\nCode\n#Data Import Part 3\n# Annual Database Service by Agency 2023\n# \nensure_package(readr)\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n  DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                      destfile=NTD_SERVICE_FILE, \n                      method=\"curl\")\n  \n  if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n    cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\n#Clean up data\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n  mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n  rename(Agency = agency, \n         City   = max_city, \n         State  = max_state,\n         UPT    = sum_unlinked_passenger_trips_upt, \n         MILES  = sum_passenger_miles) |&gt;\n  select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n  filter(MILES &gt; 0)\n\n\n ### Initial Analysis of NTD Service Data\nThe following data tables provide insights into the analysis of the NTD Service Data. The tables answer the following questions:\n\n\n1. Which transit service has the most UPT annually?\nThe transit service with the most UPT annually is “MTA New York City Transit” with a total of 2,632,003,044 UPT. \n\n\nCode\n#Task 4: Explore NTD Service Data\n#Which transit service has the most UPT annually?\nNTD_SERVICE_HighUPT &lt;- NTD_SERVICE |&gt;\n  # Find the transit service with the most UPT annually\n  group_by(Agency, City, State) |&gt;\n  summarize(Total_UPT = sum(UPT, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(Total_UPT)) |&gt;\n  slice(1)|&gt;  # Get the transit service with the most UPT\n  mutate(Total_UPT = scales::comma(Total_UPT))  # Add commas to Total UPT\n# Display the result\ndatatable(NTD_SERVICE_HighUPT, caption = \"Transit Service with the most YPT annually\" , rownames = FALSE) |&gt;\n  formatStyle(\"Total_UPT\",\n              backgroundColor = \"lightgray\", \n              fontWeight = 'bold') \n\n\n\n\n\n\n ### 2. What is the average trip length of a trip on MTA New York City Transit in miles?\nThe follow table shows the summed average trip length of a trip on MTA New York City Transit in miles. \n\n\nCode\n# Filter the dataset for the cities of interest (Brooklyn, New York, Staten Island)\n# and for the specific agency \"MTA New York City Transit\"\nmta_nyc_trip_data &lt;- NTD_SERVICE |&gt;\n  filter(Agency == \"MTA New York City Transit\")  # Filtering for MTA New York City Transit in the Agency column\n\n# Check the unique values in the Agency column to ensure it's correctly filtering\nunique(mta_nyc_trip_data$Agency)\n\n\n[1] \"MTA New York City Transit\"\n\n\nCode\n# Calculate the average trip length ( 'MILES' is the trip length column)\naverage_trip_length &lt;- mta_nyc_trip_data |&gt;\n  group_by(Agency) |&gt;\n  summarize(average_trip_length = mean(MILES, na.rm = TRUE))\n\n# Print the average trip length\n#print(average_trip_length)\n# Create a formatted datatable for the result\ndatatable(average_trip_length, caption = \"The average trip length of a trip on MTA New York City Transit in miles\", \n          rownames = FALSE, \n          options = list(pageLength = 5, dom = 't')) |&gt;\n  formatRound(columns = \"average_trip_length\", digits = 0) |&gt;\n  formatStyle(\"average_trip_length\", \n              backgroundColor = styleInterval(0, c(\"lightgreen\", \"lightblue\")),\n              fontWeight = 'bold', \n              color = \"black\")\n\n\n\n\n\n\n ### 3. Which transit service in NYC has the longest average trip length?\nThe following table shows the transit services with the longest summed average trip length in NYC. \n\n\nCode\n#Which transit service in NYC has the longest average trip length?\n\nlongest_avg_trip_length &lt;- NTD_SERVICE |&gt;\n  filter(City %in% c(\"Brooklyn\", \"New York\", \"Staten Island\")) |&gt;\n  group_by(Agency) |&gt;\n  summarize(average_trip_length = mean(MILES, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(average_trip_length))  # Sort by the longest average trip length\n\n# Print the transit service with the longest average trip length\n#print(longest_avg_trip_length)\n\n# Create a formatted datatable for the result\ndatatable(longest_avg_trip_length, caption = \"Transit Services with Longest Average Trip Length in NYC\", \n          rownames = FALSE, \n          options = list(pageLength = 5, dom = 't')) |&gt;\n  formatRound(columns = \"average_trip_length\", digits = 0) |&gt;\n  formatStyle(\"average_trip_length\", \n              backgroundColor = styleInterval(0, c(\"lightgreen\", \"lightblue\")),\n              fontWeight = 'bold', \n              color = \"black\")\n\n\n\n\n\n\n ### 4. Which state has the fewest total miles traveled by public transit?\nNew Hampshire has the fewest total miles traveled by public transit. \n\n\nCode\n#Which state has the fewest total miles traveled by public transit?\nstate_total_miles &lt;- NTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarize(total_miles = sum(MILES, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(total_miles)  # Sort in ascending order to find the state with the fewest miles\n\n# Get the state with the fewest miles\nstate_with_fewest_miles &lt;- state_total_miles[1, ]  # The first row will be the state with the fewest miles\n\n# Print the state with the fewest miles\n#print(state_with_fewest_miles)\ndatatable(state_with_fewest_miles, \n          caption = \"State with the Fewest Total Miles Traveled by Public Transit\",\n          rownames = FALSE, \n          options = list(pageLength = 5, dom = 't')) |&gt;\n  formatRound(columns = \"total_miles\", digits = 0) |&gt;\n  formatStyle(\"total_miles\", \n              backgroundColor = styleInterval(0, c(\"lightblue\", \"lightgreen\")),\n              fontWeight = 'bold', \n              color = \"black\")\n\n\n\n\n\n\n\n\n\n5. Are all states represented in this data? If no, which ones are missing?\nThe following table shows the missing states and their abbreviations in the data. \n\n\nCode\n#Are all states represented in this data? If no, which ones are missing?\n# Get the list of all U.S. state abbreviations\nall_state_abb &lt;- state.abb\n\n# Get the list of unique state abbreviations present in your dataset\nstates_in_data &lt;- unique(NTD_SERVICE$State)\n\n# Find missing state abbreviations by comparing with all_state_abb\nmissing_abb_states &lt;- setdiff(all_state_abb, states_in_data)\n\n# Combine the missing state abbreviations and their full names into one data frame\nmissing_combined &lt;- data.frame(\n  Missing_State_Abbreviation = missing_abb_states,\n  Missing_State_Full_Name = state.name[match(missing_abb_states, state.abb)]  # Mapping abbreviations to full names\n)\n\n# Display the result in a formatted datatable\ndatatable(missing_combined, \n          caption = \"Missing States in the Data (Abbreviation and Full Name)\",\n          rownames = FALSE, \n          options = list(pageLength = nrow(missing_combined), dom = 't')) |&gt;\n  formatStyle(c(\"Missing_State_Full_Name\", \"Missing_State_Abbreviation\"), \n              backgroundColor = styleInterval(0, c(\"lightblue\", \"lightcoral\")),\n              fontWeight = 'bold', \n              color = \"black\")\n\n\n\n\n\n\n ### Calculate Emissions and Normalize Emissions to Transit Usage\nThe following code calculates emissions based on the NTD Energy data and normalizes emissions to transit usage.\nThe first step was to merge the three datasets: NTD Service, NTD Energy, and EIA SEP Report. The emissions were then calculated based on the energy sources used by the transit agencies. The emissions were normalized to transit usage and categorized based on specific criteria.\n\n\nCode\n#Task 5: Calculate Emissions\n\n#Joining the three datasets\n# Step 1: Left join NTD_SERVICE and NTD_ENERGY by NTD ID\nmerged_data &lt;- left_join(NTD_SERVICE, NTD_ENERGY, by = \"NTD ID\")\n\n# Step 2: Left join with EIA_SEP_REPORT by NTD_SERVICE's \"State\" column\nmerged_data &lt;- left_join(merged_data, EIA_SEP_REPORT, by = \"State\")\n\n# Step 4: Ensure the required columns are included\nmerged_data &lt;- merged_data |&gt;\n  select(Agency, Mode, `NTD ID`, City, State, UPT, MILES, \n         `Bio-Diesel`, `Bunker Fuel`, `C Natural Gas`, `Diesel Fuel`, \n         `Electric Battery`, `Electric Propulsion`, `Ethanol`, \n         `Methonal`, `Gasoline`, `Hydrogen`, `Kerosene`, \n         `Liquified Nat Gas`, `Liquified Petroleum Gas`, \n         `Pounds of CO2 Emitted per MWh of Electricity Produced`)\n\n# Step 5: View the data\n#View(merged_data)\n\n\n ### How we calculated Total Emissions This is found in the hidden code cell below. \n\n\nCode\n#Calculate Total Emissions\ntotal_emissions_data &lt;- merged_data |&gt;\n  mutate(\n    total_emissions = (\n      `Bio-Diesel` *22.45 + \n        `Bunker Fuel` * 24.78 + \n        `C Natural Gas` * 18.32 + \n        `Diesel Fuel` * 22.45 + \n        `Electric Battery` * 0 + \n        `Electric Propulsion` * 0 + \n        `Ethanol` * 19.94 + \n        `Methonal` * 19.94  + \n        `Gasoline` * 20.86 +\n        `Hydrogen` *  0 + \n        `Kerosene` * 21.78 + \n        `Liquified Nat Gas` * 120.85 + \n        `Liquified Petroleum Gas` * 33.04\n    )\n  )\n\n#View(total_emissions_data)\n\n\n ### Normalize Emissions to Transit Usage (UPT and Passenger Mile)\nThe following data tables provide insights into the normalization of emissions to transit usage based on Unlinked Passenger Trips (UPT) and Passenger Miles. The tables categorize emissions based on specific criteria.\n ### Emissions per UPT Calculation \n\n\nCode\n# Task 6: Normalize Emissions to Transit Usage\n# Compute the emission per UPT\nemission_per_UPT &lt;- total_emissions_data |&gt;\n  mutate(per_UPT = total_emissions / UPT)\n\n# Now, calculate the categorization of per_UPT\nsum_UPT &lt;- emission_per_UPT |&gt;\n  mutate(\n    calc_cat_UPT = case_when(\n      is.na(per_UPT) ~ \"Unknown\",  # Handle NA values first\n      per_UPT &lt; 2 ~ \"Small\",       # Values less than 2\n      per_UPT &gt;= 2 & per_UPT &lt;= 4 ~ \"Medium\",  # Values between 2 and 4\n      per_UPT &gt; 4 ~ \"Large\",       # Values greater than 4\n      TRUE ~ \"Unknown\"             # Fallback if none of the conditions are met\n    )\n  )\n\n# View the result\n#View(sum_UPT)\n#Display results in a datatable\n## Create the datatable\ndatatable(sum_UPT, \n          caption = \"Categorization of Emissions per UPT\", \n          rownames = FALSE, \n          options = list(pageLength = 10)) |&gt;\n  formatStyle(\"calc_cat_UPT\", \n              backgroundColor = styleEqual(c(\"Small\", \"Medium\", \"Large\", \"Unknown\"), \n                                           c(\"lightgreen\", \"lightyellow\", \"lightblue\", \"lightgray\")),\n              fontWeight = 'bold', \n              color = \"black\")\n\n\n\n\n\n\n ### Emissions per Passenger Mile Calculation \n\n\nCode\n# Task: Compute the emission per passenger mile\nemission_per_passmile &lt;- total_emissions_data |&gt;\n  mutate(per_pass_mile = total_emissions / MILES)\n\n# Now, calculate the emission per passenger mile\nsum_passmile &lt;- emission_per_passmile |&gt;\n  mutate(\n    calc_cat_passmile = case_when(\n      is.na(per_pass_mile) ~ \"Unknown\",  # Handle NA values first\n      per_pass_mile &lt; 0.1 ~ \"Small\",     # Emissions less than 0.1\n      per_pass_mile &gt;= 0.1 & per_pass_mile &lt;= 0.3 ~ \"Medium\",  # Emissions between 0.1 and 0.3\n      per_pass_mile &gt; 0.3 ~ \"Large\",     # Emissions greater than 0.3\n      TRUE ~ \"Unknown\"                   # Fallback if none of the conditions are met\n    )\n  )\n\n# View the result\n#View(sum_passmile)\n\n# Create the datatable for the 'sum_passmile' dataset\ndatatable(sum_passmile, \n          caption = \"Emissions per Passenger Mile by Agency\", \n          rownames = FALSE, \n          options = list(pageLength = 10, dom = 't')) |&gt;\n  formatStyle(\"calc_cat_passmile\", \n              backgroundColor = styleEqual(c(\"Small\", \"Medium\", \"Large\", \"Unknown\"), \n                                           c(\"lightgreen\", \"lightyellow\", \"lightblue\", \"lightgray\")),\n              fontWeight = 'bold', \n              color = \"black\") |&gt;\n  formatRound(columns = c(\"per_pass_mile\"),\n              digits = 3)  # Format the emission per passenger mile column to 3 decimal places\n\n\n\n\n\n\n ### Agency-Level Emissions Analysis\nTo create this analysis, the total emissions, emissions per UPT, and emissions per passenger mile were computed at the agency level. The emissions were then categorized based on specific criteria. The analysis identified the greenest transit agency based on the lowest emissions per UPT.\nThe range for the categorization of emissions per UPT and emissions per passenger mile was as follows:  - Small: Less than 2  - Medium: Between 2 and 4  - Large: Greater than 4\n\n\nCode\n# Compute Total Emissions, Emissions per UPT, and Emissions per Passenger Mile at the Agency Level\nagency_emissions &lt;- total_emissions_data |&gt;\n  group_by(Agency) |&gt;\n  summarize(\n    # Sum total emissions across modes for each Agency\n    total_agency_emissions = sum(total_emissions, na.rm = TRUE),\n    \n    # Sum UPT and MILES for each Agency (needed for normalization)\n    total_UPT = sum(UPT, na.rm = TRUE),\n    total_miles = sum(MILES, na.rm = TRUE),\n    \n    # Calculate Emission per UPT for each Agency\n    emission_per_UPT = total_agency_emissions / total_UPT,\n    \n    # Calculate Emission per Passenger Mile for each Agency\n    emission_per_passmile = total_agency_emissions / total_miles\n  ) |&gt;\n  mutate(\n    # Categorize Emission per UPT (optional)\n    calc_cat_UPT = case_when(\n      is.na(emission_per_UPT) ~ \"Unknown\",\n      emission_per_UPT &lt; 2 ~ \"Small\",\n      emission_per_UPT &gt;= 2 & emission_per_UPT &lt;= 4 ~ \"Medium\",\n      emission_per_UPT &gt; 4 ~ \"Large\",\n      TRUE ~ \"Unknown\"\n    ),\n    \n    # Categorize Emission per Passenger Mile (optional)\n    calc_cat_passmile = case_when(\n      is.na(emission_per_passmile) ~ \"Unknown\",\n      emission_per_passmile &lt; 0.1 ~ \"Small\",\n      emission_per_passmile &gt;= 0.1 & emission_per_passmile &lt;= 0.3 ~ \"Medium\",\n      emission_per_passmile &gt; 0.3 ~ \"Large\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\n# View the resulting summarized and categorized emissions data for each Agency\n#View(agency_emissions)\n\n# Create the datatable for the 'agency_emissions' dataset\ndatatable(agency_emissions, \n          caption = \"Agency-Level Emissions Data\", \n          rownames = FALSE, \n          options = list(pageLength = 10, dom = 't')) |&gt;\n  formatStyle(\"calc_cat_UPT\", \n              backgroundColor = styleEqual(c(\"Small\", \"Medium\", \"Large\", \"Unknown\"), \n                                           c(\"lightgreen\", \"lightyellow\", \"lightblue\", \"lightgray\")),\n              fontWeight = 'bold', \n              color = \"black\") |&gt;\n  formatStyle(\"calc_cat_passmile\", \n              backgroundColor = styleEqual(c(\"Small\", \"Medium\", \"Large\", \"Unknown\"), \n                                           c(\"lightgreen\", \"lightyellow\", \"lightblue\", \"lightgray\")),\n              fontWeight = 'bold', \n              color = \"black\") |&gt;\n  formatRound(columns = c(\"total_agency_emissions\", \"emission_per_UPT\", \"emission_per_passmile\"),\n              digits = 2)  # Format the emissions columns to 2 decimal places\n\n\n\n\n\n\n ### Greenest Transit Agencies Analysis\nThe following code identifies the greenest transit agency based on the lowest emissions per UPT. The analysis determines the agency with the lowest emissions per UPT and displays the result in an interactive table.\nFrom the data table, we concluded that City of Portland - Portland Streetcar is the greenest transit agency with the lowest emissions per UPT.\n\n\nCode\n#Greenest Transit Agencies\n## Determine the Greenest Transit Agency (the agency with the lowest emissions per UPT)\ngreenest_agency &lt;- agency_emissions |&gt;\n  arrange(emission_per_UPT) |&gt;\n  slice(1) # Agency with the lowest emissions per UPT\n\n# Display the result\n#View(greenest_agency)\n\n# Display as an interactive table\ndatatable(greenest_agency, \n          caption = \"Greenest Transit Agency or Lowest Emissions per UPT\", \n          rownames = FALSE, \n          options = list(pageLength = 5)) |&gt;\n  formatStyle(\"emission_per_UPT\", \n              backgroundColor = styleInterval(0, c(\"lightgreen\", \"lightblue\")),\n              fontWeight = 'bold', \n              color = \"black\")\n\n\n\n\n\n\n ### Highest Electrification per Agency\nThe following code computes the percentage of electrification for each agency based on the energy sources used. The analysis identifies the agency with the highest percentage of electrification and displays the result in an interactive table.\nFrom the data table, we concluded that the City of Portland - Portland Streetcar has the highest percentage of electrification.\n\n\nCode\n# Compute the percentage of electrification for each agency\nagency_electrification &lt;- total_emissions_data |&gt;\n  mutate(\n    # Calculate total energy consumed\n    total_energy_consumed = rowSums(across(c(`Bio-Diesel`, `Bunker Fuel`, `C Natural Gas`, \n                                           `Diesel Fuel`, `Electric Battery`, `Electric Propulsion`, \n                                           `Ethanol`, `Methonal`, `Gasoline`, `Hydrogen`, \n                                           `Kerosene`, `Liquified Nat Gas`, `Liquified Petroleum Gas`)), \n                                    na.rm = TRUE),\n    \n    # Calculate percentage of energy from electric sources\n    electrification_percentage = (`Electric Battery` + `Electric Propulsion`) / total_energy_consumed * 100\n  ) |&gt;\n  group_by(Agency) |&gt;\n  summarize(\n    electrification_percentage = mean(electrification_percentage, na.rm = TRUE)\n  ) |&gt;\n  arrange(desc(electrification_percentage))\n\n# Agency with the highest percentage of electrification\nagency_with_highest_electrification &lt;- agency_electrification |&gt;\n  arrange(desc(electrification_percentage)) |&gt;\n  slice(1)\n\n# Display as an interactive table\ndatatable(agency_with_highest_electrification, \n          caption = \"Highest Percentage of Electrification\", \n          rownames = FALSE, \n          options = list(pageLength = 5)) |&gt;\n  formatStyle(\"electrification_percentage\", \n              backgroundColor = styleInterval(0, c(\"lightgreen\", \"lightblue\")),\n              fontWeight = 'bold', \n              color = \"black\")\n\n\n\n\n\n\nCode\n# Display the result\n#View(agency_with_highest_electrification)\n\n\n ### Agency with the Highest Total Emissions or the ‘Worst’ Agency\nThe following code identifies the agency with the highest total emissions. The analysis determines the agency with the highest total emissions and displays the result in an interactive table.\nFrom the data table, we concluded that the New Jersy Tranist Corp is the agency with the highest total emissions despite being considered a small agency.\n\n\nCode\n# Determine the agency with the highest total emissions\nworst_agency &lt;- agency_emissions |&gt;\n  arrange(desc(total_agency_emissions)) |&gt;\n  slice(1)\n\n# Display the result\n#View(worst_agency)\n\n# Worst Agency (highest total emissions)\nworst_agency &lt;- agency_emissions |&gt;\n  arrange(desc(total_agency_emissions)) |&gt;\n  slice(1)\n\n# Display as an interactive table\ndatatable(worst_agency, \n          caption = \"Worst Agency (Highest Total Emissions)\", \n          rownames = FALSE, \n          options = list(pageLength = 5)) |&gt;\n  formatStyle(\"total_agency_emissions\", \n              backgroundColor = styleInterval(0, c(\"lightcoral\", \"lightyellow\")),\n              fontWeight = 'bold', \n              color = \"black\")\n\n\n\n\n\n\n ### Transit Agency Categorization by Size\nThe following code categorizes transit agencies based on their total UPT size. The agencies are classified as small, medium, or large based on specific criteria. The analysis provides insights into the distribution of agencies by size category and displays the results in an interactive table.\n\n\nCode\n#Categorize Smaller agencies: Best Small Agency\n## Categorizing agencies based on total UPT size\nagency_size_category &lt;- agency_emissions |&gt;\n  mutate(\n    size_category = case_when(\n      total_UPT &lt; 500000 ~ \"Small\",\n      total_UPT &gt;= 500000 & total_UPT &lt; 20000000 ~ \"Medium\",\n      total_UPT &gt;= 20000000 ~ \"Large\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\n# Display categorized agencies in an interactive table\ndatatable(agency_size_category, \n          caption = \"Transit Agencies Categorized by Size (Total UPT)\", \n          rownames = FALSE, \n          options = list(pageLength = 10)) |&gt;\n  formatStyle(\"size_category\", \n              backgroundColor = styleEqual(c(\"Small\", \"Medium\", \"Large\"), \n                                           c(\"lightgreen\", \"lightyellow\", \"lightblue\")),\n              fontWeight = 'bold', \n              color = \"black\")\n\n\n\n\n\n\n ### Data Visualization for Transit Agencies by Size Category\nThe ggplot2 library was used to create a bar chart to visualize the distribution of transit agencies by size category. The agencies were categorized based on their total UPT size into small, medium, and large categories. The bar chart provides a visual representation of the distribution of agencies by size category.\nFrom the chart we can see that most agencies fall into the “Large” category, followed by the “Medium” category, and a few agencies in the “Small” category.Despite this the small agencies are equally the best and worst when it comes to sustainablity practices. \n\n\nCode\n#Data Visualization \nlibrary(ggplot2)\n\n# Create a bar chart to visualize the distribution of agencies by size category\nggplot(agency_size_category, aes(x = size_category, fill = size_category)) +\n  geom_bar() + # Create bars for each size category\n  labs(\n    title = \"Distribution of Transit Agencies by Size (Total UPT)\",\n    x = \"Size Category\",\n    y = \"Number of Agencies\"\n  ) +\n  scale_fill_manual(values = c(\"lightgreen\", \"lightyellow\", \"lightblue\")) + # Colors for each category\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1) # Rotate x-axis labels for readability\n  )\n\n\n\n\n\n\n\n\n\n ### Best Small Agency Analysis\nThe following code identifies the best small agency based on the lowest emissions per UPT within the small agency category. The analysis determines the agency with the lowest emissions per UPT in the small category and displays the result in an interactive table.\nThe best small agency is the “City of Fort Lauderdale” with the lowest emissions per UPT in the small category.\n\n\nCode\n## Display Best Small Agency (the winner within the \"Small\" category)\nbest_small_agency &lt;- agency_size_category |&gt;\n  filter(size_category == \"Small\") |&gt;\n  arrange(emission_per_UPT) |&gt;\n  slice(1)\n\n# Display as an interactive table\ndatatable(best_small_agency, \n          caption = \"Best Small Agency (Lowest Emissions per UPT in Small Agencies)\", \n          rownames = FALSE, \n          options = list(pageLength = 5)) |&gt;\n  formatStyle(\"emission_per_UPT\", \n              backgroundColor = styleInterval(0, c(\"lightgreen\", \"lightblue\")),\n              fontWeight = 'bold', \n              color = \"black\")\n\n\n\n\n\n\n ### Best Medium Agency The following code identifies the best medium agency based on the lowest emissions per UPT within the medium agency category. The analysis determines the agency with the lowest emissions per UPT in the medium category and displays the result in an interactive table.\nThe best medium agency is the “City of Portland” with the lowest emissions per UPT in the medium category.\n\n\nCode\n## Display Best Medium Agency (the winner within the \"Medium\" category)\nbest_med_agency &lt;- agency_size_category |&gt;\n  filter(size_category == \"Medium\") |&gt;\n  arrange(emission_per_UPT) |&gt;\n  slice(1)\n\n# Display as an interactive table\ndatatable(best_med_agency, \n          caption = \"Best Medium Agency (Lowest Emissions per UPT in Medium Agencies)\", \n          rownames = FALSE, \n          options = list(pageLength = 5)) |&gt;\n  formatStyle(\"emission_per_UPT\", \n              backgroundColor = styleInterval(0, c(\"purple\", \"navy\")),\n              fontWeight = 'bold', \n              color = \"white\")\n\n\n\n\n\n\n ### Best Large Agency The following code identifies the best large agency based on the lowest emissions per UPT within the large agency category. The analysis determines the agency with the lowest emissions per UPT in the large category and displays the result in an interactive table.\nThe best large agency is the “Port Authority Trans Hudson Corporation” with the lowest emissions per UPT in the large category.\n\n\nCode\n## Display Best Large Agency (the winner within the \"Large\" category)\nbest_large_agency &lt;- agency_size_category |&gt;\n  filter(size_category == \"Large\") |&gt;\n  arrange(emission_per_UPT) |&gt;\n  slice(1)\n\n# Display as an interactive table\ndatatable(best_large_agency, \n          caption = \"Best Large Agency (Lowest Emissions per UPT in Large Agencies)\", \n          rownames = FALSE, \n          options = list(pageLength = 5)) |&gt;\n  formatStyle(\"emission_per_UPT\", \n              backgroundColor = styleInterval(0, c(\"red\", \"orange\")),\n              fontWeight = 'bold', \n              color = \"white\")\n\n\n\n\n\n\n ### Conclusion Based on the analysis of the National Transit Database (NTD) Energy and Service data, the winners of the GTA IV Green Transit Awards have been identified. The awards recognize the transit agencies with the lowest emissions per unlinked passenger trip (UPT) and the highest electrification percentage. The analysis also categorizes transit agencies by size and identifies the best small, medium, and large agencies based on emissions per UPT. The results provide valuable insights into the environmental performance of transit agencies and highlight the efforts of the greenest and most sustainable transit providers in the United States.\n\n\nRelevant Sources and References\n\nEnergy Information Administration (EIA) - EIA Website\nNational Transit Database (NTD) - NTD Website\nU.S. Department of Transportation - DOT Website\n\n\nLast Updated: Wednesday 04 23, 2025 at 22:10PM"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini Project 4",
    "section": "",
    "text": "For this project, we collected election result data from 2020 and 2024. The ways in which we imported data from the US Census Bureau, cleaned and formatted data for analysis and extracted datafrom Wikepedia using web scraping\n\n\n\n\n\nFor this task we used the sf package to import the shapefiles with the 2nd finest resolution from teh US Census Bureau website. We used the st_read() function to read the shapefiles into R. The shapefiles contain information about the counties in the US, including their names, FIPS codes, and geometry.\n\n\n\n\nCode\n#Define the ensure_package function\n ensure_package &lt;- function(pkg) {\n   if (!requireNamespace(pkg, quietly = TRUE)) {\n     install.packages(pkg)\n   }\n   library(pkg, character.only = TRUE)\n }\n# \n# # Ensure the necessary packages are installed and loaded\n ensure_package(\"dplyr\")\n ensure_package(\"stringr\")\n ensure_package(\"tidyr\")\n ensure_package(\"httr2\")\n ensure_package(\"rvest\")\n ensure_package(\"datasets\")\n ensure_package(\"purrr\")\n ensure_package(\"DT\")\n ensure_package(\"jsonlite\")\n ensure_package(\"httr\")\n ensure_package(\"sf\")\n ensure_package(\"tigris\")\n ensure_package(\"kableExtra\")\n# # Load the kable package\n\nload_county_shapefiles &lt;- function() {\n  # Define file and directory paths\n  directory &lt;- \"data/mp04\"\n  file_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_county_5m.zip\"\n  zip_path &lt;- file.path(directory, \"cb_2023_us_county_5m.zip\")\n  shapefile_dir &lt;- file.path(directory, \"shapefiles\")\n  \n  # Create the directory if it doesn't exist\n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n  \n  # Download the file if it doesn't already exist\n  if (!file.exists(zip_path)) {\n    download.file(file_url, zip_path, mode = \"wb\")\n    message(\"File downloaded: \", zip_path)\n  } else {\n    message(\"File already exists: \", zip_path)\n  }\n  \n  # Unzip only if the .shp file doesn't already exist\n  if (!file.exists(file.path(shapefile_dir, \"cb_2023_us_county_5m.shp\"))) {\n    unzip(zip_path, exdir = shapefile_dir)\n    message(\"Unzipped to: \", shapefile_dir)\n  }\n  \n  # Read the shapefile using sf\n  shapefile_path &lt;- file.path(shapefile_dir, \"cb_2023_us_county_5m.shp\")\n  county_shapes &lt;- sf::st_read(shapefile_path, quiet = TRUE)\n  \n  return(county_shapes)\n}\n\n# Call the function\ncounty_shapefiles &lt;- load_county_shapefiles()\n\ncounty_shapefiles &lt;- county_shapefiles |&gt;\n  rename(`County Name`= NAME,\n         `Full County Name`= NAMELSAD,\n         `State Abbrevation` = STUSPS,\n         `State`= STATE_NAME ,\n         `Area Description Code`= LSAD ,\n         `Area of Land` = ALAND ,\n         `Area of Water` = AWATER )\n#View the first few rows of the shapefiles\ncounty_shapefiles &lt;- county_shapefiles |&gt;\n  select(`County Name`, `Full County Name`, `State Abbrevation`, `State`, `Area Description Code`, `Area of Land`, `Area of Water`) |&gt;\n  slice_head(n = 10)\n#head(county_shapefiles)\n#names(county_shapefiles)\n#View(county_shapefiles)\n#Create kable of the shapefiles\nkable(county_shapefiles)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounty Name\nFull County Name\nState Abbrevation\nState\nArea Description Code\nArea of Land\nArea of Water\ngeometry\n\n\n\n\nClinch\nClinch County\nGA\nGeorgia\n06\n2110829178\n23303279\nMULTIPOLYGON (((-82.9705 30…\n\n\nGraves\nGraves County\nKY\nKentucky\n06\n1429042910\n13030079\nMULTIPOLYGON (((-88.81724 3…\n\n\nTyrrell\nTyrrell County\nNC\nNorth Carolina\n06\n1012106670\n534571696\nMULTIPOLYGON (((-76.4056 35…\n\n\nBurke\nBurke County\nND\nNorth Dakota\n06\n2858360397\n65734594\nMULTIPOLYGON (((-102.939 48…\n\n\nHarper\nHarper County\nOK\nOklahoma\n06\n2689919590\n5259447\nMULTIPOLYGON (((-100.0037 3…\n\n\nHarlan\nHarlan County\nNE\nNebraska\n06\n1433471190\n53339808\nMULTIPOLYGON (((-99.63046 4…\n\n\nBlaine\nBlaine County\nNE\nNebraska\n06\n1840709884\n9415061\nMULTIPOLYGON (((-100.2676 4…\n\n\nLake and Peninsula\nLake and Peninsula Borough\nAK\nAlaska\n04\n61724231553\n25989628379\nMULTIPOLYGON (((-157.3261 5…\n\n\nJefferson\nJefferson County\nIL\nIllinois\n06\n1479490501\n32550293\nMULTIPOLYGON (((-89.1476 38…\n\n\nFulton\nFulton County\nIN\nIndiana\n06\n954133332\n7439345\nMULTIPOLYGON (((-86.46851 4…\n\n\n\n\n\n\n\n\n\n\nThen, we import the election data from 2024 from Wikipedia. The election data contains information about the number of votes cast for each candidate in each county, as well as the total number of votes cast in each county.\n\n\n\n\nCode\n#Task 2\n#US 2024 Presidential Election Results\n###########################################################################\nscrape_state_results &lt;- function(state_name) {\n  # Format the state name into a Wikipedia URL\n  base_url &lt;- \"https://en.wikipedia.org/wiki\"\n  state_slug &lt;- gsub(\" \", \"_\", state_name)\n  # Handle special cases like Washington\n  state_slug &lt;- dplyr::case_when(\n    state_name == \"Washington\" ~ \"Washington_(state)\",\n    TRUE ~ gsub(\" \", \"_\", state_name)\n  )\n  full_url &lt;- paste0(base_url, \"/2024_United_States_presidential_election_in_\", state_slug)\n  \n  # Prepare local HTML cache directory\n  html_dir &lt;- \"data/mp04/html\"\n  if (!dir.exists(html_dir)) dir.create(html_dir, recursive = TRUE)\n  html_file &lt;- file.path(html_dir, paste0(state_slug, \".html\"))\n  \n  # Download and save HTML only if not already cached\n  if (!file.exists(html_file)) {\n    resp &lt;- request(full_url) |&gt; req_perform()\n    writeBin(resp_body_raw(resp), html_file)\n    message(\"Downloaded: \", state_name)\n  } else {\n    message(\"Using cached version for: \", state_name)\n  }\n  \n  # Load HTML\n  page &lt;- read_html(html_file)\n  \n  # Extract all tables\n  tables &lt;- page |&gt; html_elements(\"table\")\n  \n  # Find the table with \"County\", \"Parish\", or \"Borough\" in column headers\n  selected_table &lt;- NULL\n  for (tbl in tables) {\n    headers &lt;- tbl |&gt; html_elements(\"th\") |&gt; html_text(trim = TRUE)\n    if (any(grepl(\"County|Parish|Borough|Area|Region|District\", headers, ignore.case = TRUE)))  {\n      selected_table &lt;- tbl\n      break\n    }\n  }\n  \n  # Return NULL if no table found\n  if (is.null(selected_table)) {\n    warning(\"No county-level table found for: \", state_name)\n    return(NULL)\n  }\n  \n  # Convert to data frame and clean\n  df &lt;- selected_table |&gt;\n    html_table(fill = TRUE) |&gt;\n    janitor::clean_names() |&gt;\n    mutate(state = state_name)\n \n  # # Standardize column types to avoid bind_rows issues\n   df &lt;- df |&gt;\n     mutate(across(everything(), as.character))  # Convert all columns to character type\n  \n  return(df)\n}\n\nus_states &lt;- state.name\n\n# Safely attempt scraping for all states\nall_election_results &lt;- purrr::map_dfr(us_states, ~{\n  tryCatch(scrape_state_results(.x), error = function(e) {\n    message(\"Error scraping \", .x, \": \", e$message)\n    return(NULL)\n  })\n})\n# View the first few rows of the combined data frame\n#head(all_election_results)\n#sum(!is.null(all_election_results))\n#names(all_election_results)\n#View(all_election_results)\n\ncleaned_results &lt;- all_election_results |&gt;\n  filter(!is.na(county)) |&gt;\n  select(county, donald_trump_republican, kamala_harris_democratic, margin, total, state, various_candidates_other_parties)\n\ncleaned_results &lt;- cleaned_results[-1,] \n\n#rename selected columns in cleaned_results\nUS_2024_Election_Results &lt;- cleaned_results |&gt;\n  rename(`County Name` = county,\n         `Donald Trump (Republican) 2024` = donald_trump_republican,\n         `Kamala Harris (Democratic) 2024` = kamala_harris_democratic,\n         `Margin 2024` = margin,\n         `Total Votes Cast 2024` = total,\n         `State` = state,\n         `Various Candidates (Other Parties) 2024` = various_candidates_other_parties)\n#View the first few rows of the cleaned results\nUS_2024_Election_Results &lt;- US_2024_Election_Results |&gt;\n  select(`County Name`, `Donald Trump (Republican) 2024`, `Kamala Harris (Democratic) 2024`, `Margin 2024`, `Total Votes Cast 2024`, `State`, `Various Candidates (Other Parties) 2024`) |&gt;\n  slice_head(n = 10)\n#View(US_2024_Election_Results)\nkable(US_2024_Election_Results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounty Name\nDonald Trump (Republican) 2024\nKamala Harris (Democratic) 2024\nMargin 2024\nTotal Votes Cast 2024\nState\nVarious Candidates (Other Parties) 2024\n\n\n\n\nAutauga\n20,484\n7,439\n13,045\n28,281\nAlabama\n358\n\n\nBaldwin\n95,798\n24,934\n70,864\n122,249\nAlabama\n1,517\n\n\nBarbour\n5,606\n4,158\n1,448\n9,855\nAlabama\n91\n\n\nBibb\n7,572\n1,619\n5,953\n9,257\nAlabama\n66\n\n\nBlount\n25,354\n2,576\n22,778\n28,163\nAlabama\n233\n\n\nBullock\n1,101\n2,983\n-1,882\n4,111\nAlabama\n27\n\n\nButler\n5,172\n3,251\n1,921\n8,480\nAlabama\n57\n\n\nCalhoun\n34,912\n13,194\n21,718\n48,653\nAlabama\n547\n\n\nChambers\n8,711\n5,405\n3,306\n14,245\nAlabama\n129\n\n\nCherokee\n11,358\n1,553\n9,805\n13,006\nAlabama\n95\n\n\n\n\n\n\n\n\n\n\nLastly, we import the election data from 2020 from Wikipedia. The election data contains information about the number of votes cast for each candidate in each county, as well as the total number of votes cast in each county.\n\n\n\n\nCode\n#Task 3: Acquire 2020 US Presidential Election Results\n#########################################################################################################\nscrape_state_results_2020 &lt;- function(state_name) {\n  # Format the state name into a Wikipedia URL\n  base_url &lt;- \"https://en.wikipedia.org/wiki\"\n  state_slug &lt;- gsub(\" \", \"_\", state_name)\n  # Handle special cases like Washington\n  state_slug &lt;- dplyr::case_when(\n    state_name == \"Washington\" ~ \"Washington_(state)\",\n    TRUE ~ gsub(\" \", \"_\", state_name)\n  )\n  full_url &lt;- paste0(base_url, \"/2020_United_States_presidential_election_in_\", state_slug)\n  \n  # Prepare local HTML cache directory\n  html_dir &lt;- \"data/mp04/html_2020\"\n  if (!dir.exists(html_dir)) dir.create(html_dir, recursive = TRUE)\n  html_file &lt;- file.path(html_dir, paste0(state_slug, \".html\"))\n  \n  # Download and save HTML only if not already cached\n  if (!file.exists(html_file)) {\n    resp &lt;- request(full_url) |&gt; req_perform()\n    writeBin(resp_body_raw(resp), html_file)\n    message(\"Downloaded: \", state_name)\n  } else {\n    message(\"Using cached version for: \", state_name)\n  }\n  \n  # Load HTML\n  page &lt;- read_html(html_file)\n  \n  # Extract all tables\n  tables &lt;- page |&gt; html_elements(\"table\")\n  \n  # Find the table with \"County\", \"Parish\", or \"Borough\" in column headers\n  selected_table &lt;- NULL\n  for (tbl in tables) {\n    headers &lt;- tbl |&gt; html_elements(\"th\") |&gt; html_text(trim = TRUE)\n    if (any(grepl(\"County|Parish|Borough|Area|Region|District\", headers, ignore.case = TRUE)))  {\n      selected_table &lt;- tbl\n      break\n    }\n  }\n  \n  if (is.null(selected_table)) {\n    warning(\"No county-level table found for: \", state_name)\n    return(NULL)\n  }\n  \n  # Convert to data frame and clean\n  df &lt;- selected_table |&gt;\n    html_table(fill = TRUE) |&gt;\n    janitor::clean_names() |&gt;\n    mutate(state = state_name) |&gt;\n    mutate(across(everything(), as.character))\n  \n  return(df)\n}\n\nus_states &lt;- state.name\n\nall_election_results_2020 &lt;- purrr::map_dfr(us_states, ~{\n  tryCatch(scrape_state_results_2020(.x), error = function(e) {\n    message(\"Error scraping \", .x, \": \", e$message)\n    return(NULL)\n  })\n})\n#head(all_election_results_2020)\n#names(all_election_results_2020)\n#View(all_election_results_2020)\n\n#Create a kable table of the election results\n#kable(all_election_results_2020)\n\ncleaned_results_2020 &lt;- all_election_results_2020 |&gt;\n  filter(!is.na(county)) |&gt;\n  select(county, donald_trump_republican, joe_biden_democratic, margin, total, state, various_candidates_other_parties)\n\ncleaned_results_2020 &lt;- cleaned_results_2020[-1,]\n\n#rename selected columns in cleaned_results\nUS_2020_Election_Results &lt;- cleaned_results_2020 |&gt;\n  rename(`County Name` = county,\n         `Donald Trump (Republican) 2020` = donald_trump_republican,\n         `Joe Biden (Democratic) 2020` = joe_biden_democratic,\n         `Margin 2020` = margin,\n         `Total Votes Cast 2020` = total,\n         `State` = state,\n         `Various Candidates (Other Parties) 2020` = various_candidates_other_parties)\n#View the first few rows of the cleaned results\nUS_2020_Election_Results &lt;- US_2020_Election_Results |&gt;\n  select(`County Name`, `Donald Trump (Republican) 2020`, `Joe Biden (Democratic) 2020`, `Margin 2020`, `Total Votes Cast 2020`, `State`, `Various Candidates (Other Parties) 2020`) |&gt;\n  slice_head(n = 10)\n#View(US_2020_Election_Results)\n#Create a kable table of the election results\nkable(US_2020_Election_Results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounty Name\nDonald Trump (Republican) 2020\nJoe Biden (Democratic) 2020\nMargin 2020\nTotal Votes Cast 2020\nState\nVarious Candidates (Other Parties) 2020\n\n\n\n\nAutauga\n19,838\n7,503\n12,335\n27,770\nAlabama\n429\n\n\nBaldwin\n83,544\n24,578\n58,966\n109,679\nAlabama\n1,557\n\n\nBarbour\n5,622\n4,816\n806\n10,518\nAlabama\n80\n\n\nBibb\n7,525\n1,986\n5,539\n9,595\nAlabama\n84\n\n\nBlount\n24,711\n2,640\n22,071\n27,588\nAlabama\n237\n\n\nBullock\n1,146\n3,446\n-2,300\n4,613\nAlabama\n21\n\n\nButler\n5,458\n3,965\n1,493\n9,488\nAlabama\n65\n\n\nCalhoun\n35,101\n15,216\n19,885\n50,983\nAlabama\n666\n\n\nChambers\n8,753\n6,365\n2,388\n15,284\nAlabama\n166\n\n\nCherokee\n10,583\n1,624\n8,959\n12,301\nAlabama\n94\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, we merge the dataframes for analysis of the election results as a whole. We will use the inner_join function from the dplyr package to merge the dataframes. The inner_join function will keep only the rows that have matching values in both dataframes. This is important because we want to analyze only the counties that are present in both election years.\n\n\n\n\nCode\n#Join the dataframes county_shapefiles, US_2024_Election_Results, and US_2020_Election_Results\n# to create a single dataframe that contains all the information you need for analysis.\n# # Use the `inner_join` function from dplyr to merge the dataframes\n#names(county_shapefiles)\n#names(US_2024_Election_Results)\n#names(US_2020_Election_Results)\n\nmerged_data &lt;- US_2024_Election_Results |&gt;\n  inner_join(US_2020_Election_Results, by = c(\"County Name\",\"State\")) |&gt;\n  inner_join(county_shapefiles, by = c(\"County Name\",\"State\"))\n# View the first few rows of the merged data\n#View(merged_data)\n#names(merged_data)\n# Create a kable table of the merged data\n#kable(merged_data) \n\n\n\n\n\n\nThe most votes for Trump is defined as the total number of votes cast for Trump in 2024. We will calculate this for each county and then find the top ten counties with the highest values.\n\n\n\n\nCode\n# Step 1: Convert Trump 2024 vote column to numeric\ntrump_votes &lt;- merged_data |&gt;\n  mutate(`Donald Trump (Republican) 2024` = as.numeric(gsub(\",\", \"\", `Donald Trump (Republican) 2024`)))\n\n# Step 2: Find the maximum number of Trump votes\ntop10_trump_votes &lt;- trump_votes |&gt;\n  arrange(desc(`Donald Trump (Republican) 2024`)) |&gt;\n  slice_head(n = 10) |&gt;\n  select(`County Name`, State, `Donald Trump (Republican) 2024`)\n  #summarise(max_votes = max(`Donald Trump (Republican) 2024`, na.rm = TRUE))\n\n# # Step 3: Filter rows where Trump got that max number of votes\n# largest_trump_votes_2024 &lt;- largest_trump_votes_2024 |&gt;\n#   filter(`Donald Trump (Republican) 2024` == max_trump_votes$max_votes) |&gt;\n#   select(`County Name`, State, `Donald Trump (Republican) 2024`)\n\n# View the result\n#View(top10_trump_votes)\n\n#create a kable table of the top 10 counties\nkable(top10_trump_votes)\n\n\n\n\n\nCounty Name\nState\nDonald Trump (Republican) 2024\n\n\n\n\nHarris\nTexas\n722695\n\n\nTarrant\nTexas\n426626\n\n\nOakland\nMichigan\n337791\n\n\nBexar\nTexas\n337545\n\n\nDallas\nTexas\n322569\n\n\nWayne\nMichigan\n288860\n\n\nMacomb\nMichigan\n284660\n\n\nCollin\nTexas\n279534\n\n\nKing\nWashington\n252193\n\n\nDenton\nTexas\n250521\n\n\n\n\n\n\n\n\n\n\nThe most votes for Biden is defined as the total number of votes cast for Biden in 2020 divided by the total number of votes cast in 2020. We will calculate this for each county and then find the top ten counties with the highest values.\n\n\n\n\nCode\n# Which county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?\n\nlibrary(readr)\n\ntop_10_biden_fraction &lt;- merged_data |&gt;\n  mutate(\n    `Biden Votes` = parse_number(`Joe Biden (Democratic) 2020`),\n    `Total Votes` = parse_number(`Total Votes Cast 2020`),\n    `Fraction of Votes for Biden` = round(`Biden Votes` / `Total Votes`,3)\n  ) |&gt;\n  filter(!is.na(`Fraction of Votes for Biden`)) |&gt;\n  arrange(desc(`Fraction of Votes for Biden`)) |&gt;\n  slice_head(n = 10) |&gt;\n  select(`County Name`, State, `Biden Votes`, `Total Votes`, `Fraction of Votes for Biden`)\n\n#View(top_10_biden_fraction)\n#Create a kable table of the top 10 counties\n\nkable(top_10_biden_fraction)\n\n\n\n\n\n\n\n\n\n\n\n\nCounty Name\nState\nBiden Votes\nTotal Votes\nFraction of Votes for Biden\n\n\n\n\nOglala Lakota\nSouth Dakota\n2829\n3200\n0.884\n\n\nJefferson\nMississippi\n3327\n3908\n0.851\n\n\nClaiborne\nMississippi\n3772\n4449\n0.848\n\n\nMacon\nAlabama\n7108\n8723\n0.815\n\n\nGreene\nAlabama\n3884\n4775\n0.813\n\n\nHolmes\nMississippi\n6588\n8115\n0.812\n\n\nDurham\nNorth Carolina\n144688\n179914\n0.804\n\n\nTodd\nSouth Dakota\n1963\n2539\n0.773\n\n\nNoxubee\nMississippi\n4040\n5339\n0.757\n\n\nAllendale\nSouth Carolina\n2718\n3593\n0.756\n\n\n\n\n\n\n\n\n\n\nThe largest shift towards Trump is defined as the difference between the total votes cast for Trump in 2024 and the total votes cast for Trump in 2020. We will calculate this difference for each county and then find the top ten counties with the highest values.\n\n\n\n\nCode\nlargest_shift_trump &lt;- merged_data |&gt;\n  mutate(\n    `Trump Shift` = as.numeric(gsub(\",\", \"\", `Donald Trump (Republican) 2024`)) - as.numeric(gsub(\",\", \"\", `Donald Trump (Republican) 2020`))\n  ) |&gt;\n  filter(!is.na(`Trump Shift`)) |&gt;\n  arrange(desc(`Trump Shift`)) |&gt;\n  slice_head(n = 10) |&gt;\n  select(`County Name`, State, `Trump Shift`)\n#View(largest_shift_trump)\n\n#Create a kable table of largest shift towards Trump\nkable(largest_shift_trump)\n\n\n\n\n\nCounty Name\nState\nTrump Shift\n\n\n\n\nBexar\nTexas\n28927\n\n\nMontgomery\nTexas\n28582\n\n\nDenton\nTexas\n28041\n\n\nCollin\nTexas\n27216\n\n\nWayne\nMichigan\n24307\n\n\nHorry\nSouth Carolina\n22898\n\n\nHarris\nTexas\n22065\n\n\nMacomb\nMichigan\n20797\n\n\nEl Paso\nTexas\n20793\n\n\nHidalgo\nTexas\n20233\n\n\n\n\n\n\n\n\n\n\nThe largest shift towards Harris is defined as the difference between the total votes cast for Harris in 2024 and the total votes cast for Trump in 2020. We will calculate this difference for each state and then find the top ten states with the highest values.\n\n\n\n\nCode\nstate_trump_shift &lt;- merged_data |&gt;\n   mutate(\n     Trump_2024 = parse_number(`Donald Trump (Republican) 2024`),\n     Trump_2020 = parse_number(`Donald Trump (Republican) 2020`)\n   ) |&gt;\n   group_by(State) |&gt;\n   summarise(\n     Total_Trump_2024 = sum(Trump_2024, na.rm = TRUE),\n     Total_Trump_2020 = sum(Trump_2020, na.rm = TRUE),\n     Trump_Vote_Shift = Total_Trump_2024 - Total_Trump_2020\n   ) |&gt;\n   arrange(desc(Trump_Vote_Shift))|&gt;  # Smallest shift first\n   slice_head(n = 10) \n#View(state_trump_shift)\n\n#Create a kable table of the small shift towards trump in 2024\nkable(state_trump_shift)\n\n\n\n\n\nState\nTotal_Trump_2024\nTotal_Trump_2020\nTrump_Vote_Shift\n\n\n\n\nTexas\n6393597\n5890347\n503250\n\n\nMichigan\n2816636\n2649864\n166772\n\n\nNorth Carolina\n2898423\n2758775\n139648\n\n\nSouth Carolina\n1483747\n1385103\n98644\n\n\nWisconsin\n1697626\n1610184\n87442\n\n\nNew Jersey\n1968215\n1883313\n84902\n\n\nMassachusetts\n1251303\n1167202\n84101\n\n\nMinnesota\n1516432\n1481537\n34895\n\n\nNew Hampshire\n395523\n365660\n29863\n\n\nIowa\n927019\n897672\n29347\n\n\n\n\n\n\n\n\n\n\nThe largest county is defined as the one with the largest area of land. We will filter the data to find the county with the maximum value in the Area of Land column.\n\n\n\n\nCode\n#Find the largest county by area\nlargest_county_by_area &lt;- merged_data |&gt;\n  # create a new column for area of land\n  mutate(`Area of Land` = as.numeric(`Area of Land`)) |&gt; \n  # filter out counties with NA area\n  filter(!is.na(`Area of Land`)) |&gt;\n  # Order by area of land in descending order\n  arrange(desc(`Area of Land`)) |&gt;\n  # Select the top county\n  slice_head(n = 1) |&gt;\n  # Select relevant columns\n  select(`County Name`, State, `Area of Land`)\n# Convert area from square meters to square miles\nlargest_county_by_area &lt;- largest_county_by_area |&gt;\n  mutate(`Area (sq mi)` = round(`Area of Land` / 2.59e+6, 2))\n#View(largest_county_by_area)\n\n#Create a kable table of the largest county by area\nkable(largest_county_by_area)\n\n\n\n\n\nCounty Name\nState\nArea of Land\nArea (sq mi)\n\n\n\n\nSweetwater\nWyoming\n27005754360\n10426.93\n\n\n\n\n\n\n\n\n\n\nVoter density is defined as the total number of votes cast in 2020 divided by the land area of the county. We will calculate this for each county and then find the county with the maximum value.\n\n\nThe highest voter denstiry in 2020 was in St. Louis, Missouri. The total votes was 536446 and the land area was 61.71937 square miles. The voter density was 8691.696 voters per square mile.\n\n\n\n\nCode\nhighest_voter_density_2020 &lt;- merged_data |&gt;\n  mutate(\n    `Total Votes 2020` = parse_number(`Total Votes Cast 2020`),\n    `Land Area (sq mi)` = as.numeric(`Area of Land`) / 2.59e+6,  # Convert m² to mi²\n    `Voter Density (2020)` = `Total Votes 2020` / `Land Area (sq mi)`\n  ) |&gt;\n  filter(!is.na(`Voter Density (2020)`), `Land Area (sq mi)` &gt; 0) |&gt;\n  arrange(desc(`Voter Density (2020)`)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(`County Name`, State, `Total Votes 2020`, `Land Area (sq mi)`, `Voter Density (2020)`)\n#View(highest_voter_density_2020)\nkable(highest_voter_density_2020)\n\n\n\n\n\n\n\n\n\n\n\n\nCounty Name\nState\nTotal Votes 2020\nLand Area (sq mi)\nVoter Density (2020)\n\n\n\n\n\n\n\n\n\n\n\n\nThe largest increase in voter turnout is defined as the difference between the total votes cast in 2024 and the total votes cast in 2020. We will calculate this difference for each county and then find the county with the maximum value.\n\n\n\n\nCode\nlargest_turnout_increase_2024 &lt;- merged_data |&gt;\n  mutate(\n    `Total Votes 2024` = parse_number(`Total Votes Cast 2024`),\n    `Total Votes 2020` = parse_number(`Total Votes Cast 2020`)\n  ) |&gt;\n  filter(!is.na(`Total Votes 2024`), !is.na(`Total Votes 2020`)) |&gt;\n  mutate(\n    `Turnout Increase` = `Total Votes 2024` - `Total Votes 2020`\n  ) |&gt;\n  arrange(desc(`Turnout Increase`)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(`County Name`, State, `Turnout Increase`)\n#View(largest_turnout_increase_2024)\n\n#Create a kable table of the largest increase in voter turnout in 2024\nkable(largest_turnout_increase_2024)\n\n\n\n\n\nCounty Name\nState\nTurnout Increase\n\n\n\n\nMontgomery\nTexas\n35715\n\n\n\n\n\n\n\n\n\n\n\nSteps taken to reproduce the NYT shift figure: We computed the shift as a percentage of votes cast rightwards for each county. Then, we modified the geometry of the shapefiles to reposition Alaska and Hawaii. Next, we computed the centroid of each county. In additon, we added an arrow for each county, located at its centroid. Finally, plot the results using ggplot2 with the modified geometery.\n\n\n\n\nCode\n# Load required packages\nlibrary(dplyr)\nlibrary(readr)\nlibrary(sf)\nlibrary(tigris)\nlibrary(ggplot2)\nlibrary(units)\n\n\nshift_data &lt;- merged_data |&gt;\n  mutate(\n    trump_2020 = parse_number(`Donald Trump (Republican) 2020`),\n    trump_2024 = parse_number(`Donald Trump (Republican) 2024`),\n    total_2020 = parse_number(`Total Votes Cast 2020`),\n    total_2024 = parse_number(`Total Votes Cast 2024`),\n    pct_trump_2020 = trump_2020 / total_2020,\n    pct_trump_2024 = trump_2024 / total_2024,\n    shift = pct_trump_2024 - pct_trump_2020\n  )\nshift_data &lt;- st_as_sf(shift_data)  # Re-attach geometry and sf methods\n\nreposition_states &lt;- function(geo_data) {\n  # Extract geometry\n  geo_data$geometry_orig &lt;- st_geometry(geo_data)\n  \n  # Reposition Hawaii\n  geo_data$geometry[geo_data$State == \"Hawaii\"] &lt;- \n    geo_data$geometry_orig[geo_data$State == \"Hawaii\"] * 0.35 + c(40, -15)\n  \n  # Reposition Alaska\n  geo_data$geometry[geo_data$State == \"Alaska\"] &lt;- \n    geo_data$geometry_orig[geo_data$State == \"Alaska\"] * 0.35 + c(55, -15)\n  \n  geo_data &lt;- st_as_sf(geo_data)\n  geo_data$geometry_orig &lt;- NULL  # Clean up if needed\n  return(geo_data)\n}\nshifted_geometries &lt;- reposition_states(shift_data)\n\nshifted_geometries &lt;- shifted_geometries |&gt;\n  mutate(centroid = st_centroid(geometry)) |&gt;\n  mutate(\n    lon = st_coordinates(centroid)[, 1],\n    lat = st_coordinates(centroid)[, 2]\n  )\n\narrow_data &lt;- shifted_geometries |&gt;\n  mutate(\n    dx = shift * 5,  # scale factor; adjust as needed\n    dy = 0,\n    xend = lon + dx,\n    yend = lat + dy\n  )\n# Add dx and dy based on shift\narrow_data &lt;- shifted_geometries |&gt;\n  filter(!is.na(lon), !is.na(lat), !is.na(shift)) |&gt;\n  mutate(\n    dx = shift * 5,  # adjust scaling factor as needed for visibility\n    dy = 0  # arrows go horizontally (right = red shift, left = blue shift)\n  )\n\nggplot() +\n  geom_sf(data = shifted_geometries, fill = \"gray95\", color = \"white\", size = 0.1) +\n  geom_segment(\n    data = arrow_data,\n    aes(x = lon, y = lat, xend = lon + dx, yend = lat + dy, color = shift),\n    arrow = arrow(length = unit(0.1, \"inches\")),\n    linewidth = 0.3\n  ) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +\n  theme_minimal() +\n  labs(\n    title = \"Shift in Republican Vote Share (2020 → 2024)\",\n    subtitle = \"Arrows point rightward for increased Trump support, leftward for decreased\"\n  )"
  },
  {
    "objectID": "mp04.html#initial-analysis-questions",
    "href": "mp04.html#initial-analysis-questions",
    "title": "Mini Project 4",
    "section": "",
    "text": "First, we merge the dataframes for analysis of the election results as a whole. We will use the inner_join function from the dplyr package to merge the dataframes. The inner_join function will keep only the rows that have matching values in both dataframes. This is important because we want to analyze only the counties that are present in both election years.\n\n\n\n\nCode\n#Join the dataframes county_shapefiles, US_2024_Election_Results, and US_2020_Election_Results\n# to create a single dataframe that contains all the information you need for analysis.\n# # Use the `inner_join` function from dplyr to merge the dataframes\n#names(county_shapefiles)\n#names(US_2024_Election_Results)\n#names(US_2020_Election_Results)\n\nmerged_data &lt;- US_2024_Election_Results |&gt;\n  inner_join(US_2020_Election_Results, by = c(\"County Name\",\"State\")) |&gt;\n  inner_join(county_shapefiles, by = c(\"County Name\",\"State\"))\n# View the first few rows of the merged data\n#View(merged_data)\n#names(merged_data)\n# Create a kable table of the merged data\n#kable(merged_data) \n\n\n\n\n\n\nThe most votes for Trump is defined as the total number of votes cast for Trump in 2024. We will calculate this for each county and then find the top ten counties with the highest values.\n\n\n\n\nCode\n# Step 1: Convert Trump 2024 vote column to numeric\ntrump_votes &lt;- merged_data |&gt;\n  mutate(`Donald Trump (Republican) 2024` = as.numeric(gsub(\",\", \"\", `Donald Trump (Republican) 2024`)))\n\n# Step 2: Find the maximum number of Trump votes\ntop10_trump_votes &lt;- trump_votes |&gt;\n  arrange(desc(`Donald Trump (Republican) 2024`)) |&gt;\n  slice_head(n = 10) |&gt;\n  select(`County Name`, State, `Donald Trump (Republican) 2024`)\n  #summarise(max_votes = max(`Donald Trump (Republican) 2024`, na.rm = TRUE))\n\n# # Step 3: Filter rows where Trump got that max number of votes\n# largest_trump_votes_2024 &lt;- largest_trump_votes_2024 |&gt;\n#   filter(`Donald Trump (Republican) 2024` == max_trump_votes$max_votes) |&gt;\n#   select(`County Name`, State, `Donald Trump (Republican) 2024`)\n\n# View the result\n#View(top10_trump_votes)\n\n#create a kable table of the top 10 counties\nkable(top10_trump_votes)\n\n\n\n\n\nCounty Name\nState\nDonald Trump (Republican) 2024\n\n\n\n\nHarris\nTexas\n722695\n\n\nTarrant\nTexas\n426626\n\n\nOakland\nMichigan\n337791\n\n\nBexar\nTexas\n337545\n\n\nDallas\nTexas\n322569\n\n\nWayne\nMichigan\n288860\n\n\nMacomb\nMichigan\n284660\n\n\nCollin\nTexas\n279534\n\n\nKing\nWashington\n252193\n\n\nDenton\nTexas\n250521\n\n\n\n\n\n\n\n\n\n\nThe most votes for Biden is defined as the total number of votes cast for Biden in 2020 divided by the total number of votes cast in 2020. We will calculate this for each county and then find the top ten counties with the highest values.\n\n\n\n\nCode\n# Which county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?\n\nlibrary(readr)\n\ntop_10_biden_fraction &lt;- merged_data |&gt;\n  mutate(\n    `Biden Votes` = parse_number(`Joe Biden (Democratic) 2020`),\n    `Total Votes` = parse_number(`Total Votes Cast 2020`),\n    `Fraction of Votes for Biden` = round(`Biden Votes` / `Total Votes`,3)\n  ) |&gt;\n  filter(!is.na(`Fraction of Votes for Biden`)) |&gt;\n  arrange(desc(`Fraction of Votes for Biden`)) |&gt;\n  slice_head(n = 10) |&gt;\n  select(`County Name`, State, `Biden Votes`, `Total Votes`, `Fraction of Votes for Biden`)\n\n#View(top_10_biden_fraction)\n#Create a kable table of the top 10 counties\n\nkable(top_10_biden_fraction)\n\n\n\n\n\n\n\n\n\n\n\n\nCounty Name\nState\nBiden Votes\nTotal Votes\nFraction of Votes for Biden\n\n\n\n\nOglala Lakota\nSouth Dakota\n2829\n3200\n0.884\n\n\nJefferson\nMississippi\n3327\n3908\n0.851\n\n\nClaiborne\nMississippi\n3772\n4449\n0.848\n\n\nMacon\nAlabama\n7108\n8723\n0.815\n\n\nGreene\nAlabama\n3884\n4775\n0.813\n\n\nHolmes\nMississippi\n6588\n8115\n0.812\n\n\nDurham\nNorth Carolina\n144688\n179914\n0.804\n\n\nTodd\nSouth Dakota\n1963\n2539\n0.773\n\n\nNoxubee\nMississippi\n4040\n5339\n0.757\n\n\nAllendale\nSouth Carolina\n2718\n3593\n0.756\n\n\n\n\n\n\n\n\n\n\nThe largest shift towards Trump is defined as the difference between the total votes cast for Trump in 2024 and the total votes cast for Trump in 2020. We will calculate this difference for each county and then find the top ten counties with the highest values.\n\n\n\n\nCode\nlargest_shift_trump &lt;- merged_data |&gt;\n  mutate(\n    `Trump Shift` = as.numeric(gsub(\",\", \"\", `Donald Trump (Republican) 2024`)) - as.numeric(gsub(\",\", \"\", `Donald Trump (Republican) 2020`))\n  ) |&gt;\n  filter(!is.na(`Trump Shift`)) |&gt;\n  arrange(desc(`Trump Shift`)) |&gt;\n  slice_head(n = 10) |&gt;\n  select(`County Name`, State, `Trump Shift`)\n#View(largest_shift_trump)\n\n#Create a kable table of largest shift towards Trump\nkable(largest_shift_trump)\n\n\n\n\n\nCounty Name\nState\nTrump Shift\n\n\n\n\nBexar\nTexas\n28927\n\n\nMontgomery\nTexas\n28582\n\n\nDenton\nTexas\n28041\n\n\nCollin\nTexas\n27216\n\n\nWayne\nMichigan\n24307\n\n\nHorry\nSouth Carolina\n22898\n\n\nHarris\nTexas\n22065\n\n\nMacomb\nMichigan\n20797\n\n\nEl Paso\nTexas\n20793\n\n\nHidalgo\nTexas\n20233\n\n\n\n\n\n\n\n\n\n\nThe largest shift towards Harris is defined as the difference between the total votes cast for Harris in 2024 and the total votes cast for Trump in 2020. We will calculate this difference for each state and then find the top ten states with the highest values.\n\n\n\n\nCode\nstate_trump_shift &lt;- merged_data |&gt;\n   mutate(\n     Trump_2024 = parse_number(`Donald Trump (Republican) 2024`),\n     Trump_2020 = parse_number(`Donald Trump (Republican) 2020`)\n   ) |&gt;\n   group_by(State) |&gt;\n   summarise(\n     Total_Trump_2024 = sum(Trump_2024, na.rm = TRUE),\n     Total_Trump_2020 = sum(Trump_2020, na.rm = TRUE),\n     Trump_Vote_Shift = Total_Trump_2024 - Total_Trump_2020\n   ) |&gt;\n   arrange(desc(Trump_Vote_Shift))|&gt;  # Smallest shift first\n   slice_head(n = 10) \n#View(state_trump_shift)\n\n#Create a kable table of the small shift towards trump in 2024\nkable(state_trump_shift)\n\n\n\n\n\nState\nTotal_Trump_2024\nTotal_Trump_2020\nTrump_Vote_Shift\n\n\n\n\nTexas\n6393597\n5890347\n503250\n\n\nMichigan\n2816636\n2649864\n166772\n\n\nNorth Carolina\n2898423\n2758775\n139648\n\n\nSouth Carolina\n1483747\n1385103\n98644\n\n\nWisconsin\n1697626\n1610184\n87442\n\n\nNew Jersey\n1968215\n1883313\n84902\n\n\nMassachusetts\n1251303\n1167202\n84101\n\n\nMinnesota\n1516432\n1481537\n34895\n\n\nNew Hampshire\n395523\n365660\n29863\n\n\nIowa\n927019\n897672\n29347\n\n\n\n\n\n\n\n\n\n\nThe largest county is defined as the one with the largest area of land. We will filter the data to find the county with the maximum value in the Area of Land column.\n\n\n\n\nCode\n#Find the largest county by area\nlargest_county_by_area &lt;- merged_data |&gt;\n  # create a new column for area of land\n  mutate(`Area of Land` = as.numeric(`Area of Land`)) |&gt; \n  # filter out counties with NA area\n  filter(!is.na(`Area of Land`)) |&gt;\n  # Order by area of land in descending order\n  arrange(desc(`Area of Land`)) |&gt;\n  # Select the top county\n  slice_head(n = 1) |&gt;\n  # Select relevant columns\n  select(`County Name`, State, `Area of Land`)\n# Convert area from square meters to square miles\nlargest_county_by_area &lt;- largest_county_by_area |&gt;\n  mutate(`Area (sq mi)` = round(`Area of Land` / 2.59e+6, 2))\n#View(largest_county_by_area)\n\n#Create a kable table of the largest county by area\nkable(largest_county_by_area)\n\n\n\n\n\nCounty Name\nState\nArea of Land\nArea (sq mi)\n\n\n\n\nSweetwater\nWyoming\n27005754360\n10426.93\n\n\n\n\n\n\n\n\n\n\nVoter density is defined as the total number of votes cast in 2020 divided by the land area of the county. We will calculate this for each county and then find the county with the maximum value.\n\n\nThe highest voter denstiry in 2020 was in St. Louis, Missouri. The total votes was 536446 and the land area was 61.71937 square miles. The voter density was 8691.696 voters per square mile.\n\n\n\n\nCode\nhighest_voter_density_2020 &lt;- merged_data |&gt;\n  mutate(\n    `Total Votes 2020` = parse_number(`Total Votes Cast 2020`),\n    `Land Area (sq mi)` = as.numeric(`Area of Land`) / 2.59e+6,  # Convert m² to mi²\n    `Voter Density (2020)` = `Total Votes 2020` / `Land Area (sq mi)`\n  ) |&gt;\n  filter(!is.na(`Voter Density (2020)`), `Land Area (sq mi)` &gt; 0) |&gt;\n  arrange(desc(`Voter Density (2020)`)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(`County Name`, State, `Total Votes 2020`, `Land Area (sq mi)`, `Voter Density (2020)`)\n#View(highest_voter_density_2020)\nkable(highest_voter_density_2020)\n\n\n\n\n\n\n\n\n\n\n\n\nCounty Name\nState\nTotal Votes 2020\nLand Area (sq mi)\nVoter Density (2020)\n\n\n\n\n\n\n\n\n\n\n\n\nThe largest increase in voter turnout is defined as the difference between the total votes cast in 2024 and the total votes cast in 2020. We will calculate this difference for each county and then find the county with the maximum value.\n\n\n\n\nCode\nlargest_turnout_increase_2024 &lt;- merged_data |&gt;\n  mutate(\n    `Total Votes 2024` = parse_number(`Total Votes Cast 2024`),\n    `Total Votes 2020` = parse_number(`Total Votes Cast 2020`)\n  ) |&gt;\n  filter(!is.na(`Total Votes 2024`), !is.na(`Total Votes 2020`)) |&gt;\n  mutate(\n    `Turnout Increase` = `Total Votes 2024` - `Total Votes 2020`\n  ) |&gt;\n  arrange(desc(`Turnout Increase`)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(`County Name`, State, `Turnout Increase`)\n#View(largest_turnout_increase_2024)\n\n#Create a kable table of the largest increase in voter turnout in 2024\nkable(largest_turnout_increase_2024)\n\n\n\n\n\nCounty Name\nState\nTurnout Increase\n\n\n\n\nMontgomery\nTexas\n35715\n\n\n\n\n\n\n\n\n\n\n\nSteps taken to reproduce the NYT shift figure: We computed the shift as a percentage of votes cast rightwards for each county. Then, we modified the geometry of the shapefiles to reposition Alaska and Hawaii. Next, we computed the centroid of each county. In additon, we added an arrow for each county, located at its centroid. Finally, plot the results using ggplot2 with the modified geometery.\n\n\n\n\nCode\n# Load required packages\nlibrary(dplyr)\nlibrary(readr)\nlibrary(sf)\nlibrary(tigris)\nlibrary(ggplot2)\nlibrary(units)\n\n\nshift_data &lt;- merged_data |&gt;\n  mutate(\n    trump_2020 = parse_number(`Donald Trump (Republican) 2020`),\n    trump_2024 = parse_number(`Donald Trump (Republican) 2024`),\n    total_2020 = parse_number(`Total Votes Cast 2020`),\n    total_2024 = parse_number(`Total Votes Cast 2024`),\n    pct_trump_2020 = trump_2020 / total_2020,\n    pct_trump_2024 = trump_2024 / total_2024,\n    shift = pct_trump_2024 - pct_trump_2020\n  )\nshift_data &lt;- st_as_sf(shift_data)  # Re-attach geometry and sf methods\n\nreposition_states &lt;- function(geo_data) {\n  # Extract geometry\n  geo_data$geometry_orig &lt;- st_geometry(geo_data)\n  \n  # Reposition Hawaii\n  geo_data$geometry[geo_data$State == \"Hawaii\"] &lt;- \n    geo_data$geometry_orig[geo_data$State == \"Hawaii\"] * 0.35 + c(40, -15)\n  \n  # Reposition Alaska\n  geo_data$geometry[geo_data$State == \"Alaska\"] &lt;- \n    geo_data$geometry_orig[geo_data$State == \"Alaska\"] * 0.35 + c(55, -15)\n  \n  geo_data &lt;- st_as_sf(geo_data)\n  geo_data$geometry_orig &lt;- NULL  # Clean up if needed\n  return(geo_data)\n}\nshifted_geometries &lt;- reposition_states(shift_data)\n\nshifted_geometries &lt;- shifted_geometries |&gt;\n  mutate(centroid = st_centroid(geometry)) |&gt;\n  mutate(\n    lon = st_coordinates(centroid)[, 1],\n    lat = st_coordinates(centroid)[, 2]\n  )\n\narrow_data &lt;- shifted_geometries |&gt;\n  mutate(\n    dx = shift * 5,  # scale factor; adjust as needed\n    dy = 0,\n    xend = lon + dx,\n    yend = lat + dy\n  )\n# Add dx and dy based on shift\narrow_data &lt;- shifted_geometries |&gt;\n  filter(!is.na(lon), !is.na(lat), !is.na(shift)) |&gt;\n  mutate(\n    dx = shift * 5,  # adjust scaling factor as needed for visibility\n    dy = 0  # arrows go horizontally (right = red shift, left = blue shift)\n  )\n\nggplot() +\n  geom_sf(data = shifted_geometries, fill = \"gray95\", color = \"white\", size = 0.1) +\n  geom_segment(\n    data = arrow_data,\n    aes(x = lon, y = lat, xend = lon + dx, yend = lat + dy, color = shift),\n    arrow = arrow(length = unit(0.1, \"inches\")),\n    linewidth = 0.3\n  ) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +\n  theme_minimal() +\n  labs(\n    title = \"Shift in Republican Vote Share (2020 → 2024)\",\n    subtitle = \"Arrows point rightward for increased Trump support, leftward for decreased\"\n  )"
  }
]